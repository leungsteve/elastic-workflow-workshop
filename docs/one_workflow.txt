Introduction
  

This document outlines the vision, strategy, and execution plan for the One Workflow initiative. As the primary reference for all stakeholders, it details our approach, from understanding the customer opportunity and product requirements to planning delivery milestones for One Workflow.
What is One Workflow?
Accelerated by our acquisition of Keep, One Workflow is Elastic's initiative to introduce native workflow automation across our solutions. It will be the engine that powers our shift from insight to automated outcomes, faster and more intelligently than ever before, across Search, Observability, and Security.
Vision & Strategy Summary
Our mission at Elastic is to enable users to find the answers that matter from all data, in real-time and at scale. But insight alone isn't enough. The ultimate value lies in outcomes. Today, our customers face persistent alert fatigue, understaffing, and the high cost of manual, repetitive work. They're forced to bolt on external automation tools to operationalize their data and insights in Elastic. This gap has become a competitive disadvantage, leading prospects to choose more integrated platforms.


One Workflow is our response. By natively integrating workflow automation into our platform, we complete the journey from data to insight to automated outcomes. Our customers have already centralized their critical operational data on Elastic (security events, infrastructure metrics, application logs, business data/context). Now they can automate end-to-end outcomes directly where that data lives. The workflow automation capabilities we introduce will enable the automation of everything from simple, repeatable tasks to complex workflows powered by AI.


By combining our data foundation (the "senses"), our Search AI platform (the "brain"), and now workflow automation (the "hands"), we create the unified experience needed for our long-term vision: Elastic as the central nervous system for autonomous security, IT, and business operations.


This document serves as a guide for this initiative, from high-level vision to detailed execution plan.


* Vision 
* Who We’re Building For 
* Core Components of Workflow Automation 
* Use Case 
* Product Requirements 
* Milestone Plan 
* ..




Vision
  

One Workflow Vision
One Workflow Vision
Background
The Two-Vendor Problem
From Rules to Reasoning: How AI Changes Automation
Our Vision
The Path Forward
Background
The Two-Vendor Problem
Every operational team faces the same reality: exponential data growth meets linear human capacity. Security alerts multiply. Infrastructure events cascade. Business processes accumulate. The math doesn't work. Our customers solve this by combining Elastic with external automation tools. They use Elastic to find the right signal/insight, then bolt on third-party workflow automation tools to act on it.


Consider our own InfoSec team. They process ~65,000 security alerts monthly across our global infrastructure. For a team of their size, this should be impossible. Instead, by combining Elastic Security with Tines (workflow automation / SOAR tool), they've achieved something remarkable:


* Over 60,128 alerts automated monthly without human intervention
* Automation handling work that would require 94 full-time analysts
* Only 1,430 alerts (~2%) requiring human investigation
  

The results are undeniable. This same story plays out across our customer base. In fact, in March 2025, we formalized our partnership with Tines because so many customers were already combining our platforms. Customers like Texas A&M called the partnership a "game-changer," but they shouldn't need two vendors to achieve it. This two-vendor pattern reveals an uncomfortable truth: 


We excel at finding the needle in the haystack. Our customers must look elsewhere to pick it up.


In the meantime, the markets we play in have evolved. Leading platforms now make native automation table stakes. Splunk acquired Phantom for SOAR capabilities.  Datadog embeds workflow automation directly into their observability platform. Microsoft bundles Logic Apps with Sentinel. Google SecOps (formerly Chronicle) includes native SOAR. New Relic offers native workflows and is building agentic AI orchestration. Dynatrace's AutomationEngine and Davis AI enable automated remediation, etc., etc.,...


Native workflow automation has become an expected capability. Every RFP includes it. Every evaluation asks about it. The market has moved, and we must move with it. Every deal we lose to "lack of native automation" is a deal we should have won.


But moving with the market doesn't mean simply copying what others have done. The real question isn't whether we need native workflow automation; that's table stakes. The question is what kind of automation the market will demand tomorrow, not just today. 
From Rules to Reasoning: How AI Changes Automation
Traditional workflow automation has delivered massive value for operational teams, giving them tremendous leverage and scale. As we covered above, our InfoSec team's results prove this: work that would require 94 full-time analysts now runs automatically, at scale with no human intervention. No AI, no machine learning, or any other buzzword, just well-crafted workflows that codify operational expertise into repeatable logic.


However, returning to the chart earlier, examine the remaining 2.2% - specifically, the 1,430 alerts that still required human investigation. 
  



They needed context only humans could provide, judgment calls about business impact, or nuanced decision-making that couldn't be codified into rigid rules. This reveals the historic limitation of traditional workflow automation: it excels at codifying known processes but stops where human judgment begins.


That limitation is dissolving.


The AI revolution of the last couple of years isn't just improving workflow automation; it's redefining what outcomes are possible through workflow automation. Users can now embed AI agents as configurable steps within their automation, with full control over their behavior and boundaries.


Traditional automation handles the routine:
* If alert X, then enrich with data Y and create ticket Z
* Deterministic, fast, reliable


AI-enhanced automation handles the complex:
* A critical alert triggers a workflow
* The workflow invokes an AI agent configured as a "Senior SOC Analyst"
* The agent has access to specific tools: query APIs, enrichment services, even sub-workflows
* Given just the alert, it autonomously investigates: queries multiple systems, correlates events, checks threat intelligence
* It operates within user-defined boundaries: what tools it can access, what actions it can take, when to escalate
* All investigation findings flow to the next workflow steps or to human analysts


Those 1,430 alerts our InfoSec team manually investigated? Workflow with agentic steps could handle them. Not because AI replaces human judgment, but because humans can encode their expertise, give agents the same tools they use, and let intelligent automated workflows handle the investigation while maintaining oversight.


The market's expectations have shifted accordingly. They not only want workflows that automate and thereby scale repetitive work. They also want workflows that scale expertise itself.
Our Vision
We envision a world where Elastic is the single platform where data naturally flows into insight, and insight seamlessly transforms into intelligent, automated outcomes. No bolt-ons. No integration headaches. No switching between tools to get work done.
The Unified Experience
Today, our customers excel at finding critical signals in their data. Tomorrow, they'll act on those signals without leaving Elastic. A security alert won't just surface a threat; it will trigger an investigation, contain the breach, and document the response. An infrastructure anomaly won't just generate a notification; it will diagnose the root cause, execute remediation, and prevent recurrence. A business insight won't just appear in a dashboard; it will initiate processes, update systems, and drive outcomes.


This goes beyond just adding another feature. It enables us to complete the data-insight-action value chain. When our customers centralize their operational data in Elastic, they're not just storing information. They're building the foundation for automated intelligence. Every log line, metric, trace, and document becomes fuel for automation that learns, adapts, and scales with their needs.
Beyond Traditional Automation
Traditional automation handles the known unknowns: the predictable patterns and codified responses. But operational reality can be messy. It requires judgment, investigation, and adaptive reasoning. It demands automation that can “think”, not just execute.


By embedding AI as a native part of workflows, we enable our customers to scale their expertise, not just their processes. An experienced security analyst's investigation methodology becomes a reusable asset. A senior SRE's troubleshooting intuition transforms into automated diagnostics. Business logic that once required human interpretation operates autonomously while maintaining human oversight.


This is automation that brings intelligence and adaptive reasoning to operational challenges.
The Platform Advantage
Our vision leverages Elastic's unique position. We already host the data. We already power the analytics. We already deliver the insights. Adding native workflow automation completes the system:


Data Foundation: The critical operational data already lives in Elastic: security events, infrastructure metrics, application logs, business documents. This isn't data copied to an automation tool; it's automation operating directly where the data resides.


Search AI Platform: We've spent years building AI into the heart of Elastic, combining the precision of search with the intelligence of AI to help users find meaning in their data. This foundation naturally extends to power intelligent automation. When workflows need to interpret logs, understand alerts, or make decisions, they'll tap into the same Search AI infrastructure that already makes our platform smart. The intelligence is already here; we're just expanding what it can do.


Unified Interface: Users won't context-switch between their observability platform and their automation tool, or their SIEM and their SOAR. One platform, one experience, one source of truth.
Enabling Three Transformations
For Security Teams
Elastic Security becomes a complete SecOps platform. Detection naturally flows into response. Every alert can trigger investigation, enrichment, containment, and remediation. Security teams encode their playbooks once and let automation handle the execution at scale. AI-powered steps handle complex threat hunting/investigations/etc., that previously required senior analysts. The result: faster response times, consistent processes, and analysts focused on strategic security initiatives rather than repetitive tasks.
For Observability Teams
Elastic Observability evolves into a self-healing nervous system. Monitoring transforms into proactive operations. Issues are not just detected but diagnosed, remediated, and prevented. AIOps becomes reality as workflows orchestrate everything from auto-scaling to root cause analysis to preventive maintenance. AI agents investigate complex performance issues, correlating across metrics, logs, and traces to identify problems humans might miss.
For Search Users
Elastic becomes the intelligent backbone for business operations. Search results trigger actions. Insights initiate processes. Documents flow through intelligent workflows that extract, transform, classify, and route information. Knowledge workers build custom automations that connect their Elastic-powered applications to their entire business ecosystem. 
The Outcome: Operational Intelligence at Scale
When this vision becomes reality, our customers won't just have better automation. They'll operate fundamentally differently:
* From Reactive to Proactive: Issues can be resolved before they impact users
* From Manual to Intelligent: Expertise scales across the entire team
* From Isolated to Integrated: Data, insight, and action flow seamlessly
* From Rigid to Adaptive: Automation evolves with changing needs


Elastic becomes more than a platform; it becomes the operational nervous system for the modern enterprise. The senses that collect signals. The brain that processes and understands. The hands that take action. All unified, all intelligent, all under human control.


This is our vision: to make Elastic the place where operational intelligence lives and works. Where every piece of data has the potential to drive automated outcomes. Where human expertise is amplified, not replaced. Where the gap between knowing and doing disappears. That future starts with One Workflow.








Problem Statement, Target Audience & User Stories
This section outlines the strategic context for introducing a unified workflow automation capability within Elastic. Before diving into specific product requirements, it is essential to first establish a clear, shared understanding of the core problems we aim to solve, the specific audiences we are building for, and the fundamental outcomes they need to achieve. This document will cover:
* The Problem Statement: Articulating the critical challenges our users face and the capability gaps within Elastic around workflow automation.
* Target Audience: Defining the key personas whose needs will guide our development priorities.
* User Stories: Translating the needs of our audience into a set of foundational, outcome-driven goals that will serve as the "why" behind our product requirements.
Introduction
Problem Statement
Target Audience
Primary Persona: The Practitioner / Operator
Secondary Persona: The Team Lead / Architect
Tertiary Persona: The Developer / Builder
Detailed User Stories
Workflow Definition & Creation
Workflow Triggers & Execution
Flow Control & Logic
Data Handling & Transformation
Integration & Actions
Workflow Management & Governance
Problem Statement
Operations teams across Security, Observability, and IT are overwhelmed by the volume of data, alerts, and manual tasks required to maintain system health and security. This leads to:
* Analyst Burnout: Repetitive, manual investigation and triage tasks lead to fatigue and errors.
* Slow Response Times: The time required to coordinate across teams and tools for incident response or root cause analysis is too long.
* Missed Insights: Valuable signals are lost because teams lack the capacity to investigate everything.
This industry-wide challenge is amplified within the Elastic ecosystem due to a critical capability gap:
* Lack of Native Workflow Automation: There is no native way to automate workflows that chain multiple actions together. Automating a sequence (for example, enriching an alert, querying for related indicators, updating a case, and notifying a response team) requires custom scripts or external tools, creating a disjointed and inefficient experience for our users.
* Consequent Reliance on External Platforms: This lack of native orchestration directly forces our customers to augment Elastic with third-party workflow platforms. They pay other vendors to solve a problem that begins and ends with their data in Elastic, representing an opportunity cost for our business.
Target Audience
Primary Persona: The Practitioner / Operator
* Who they are: Security Analysts, Site Reliability Engineers (SREs), and DevOps Engineers who are the hands-on users of the Elastic platform.
* What they do: They spend their day triaging security alerts, investigating performance issues, responding to incidents, and running diagnostics. Their work is highly reactive and time sensitive.
* What they need: They need to automate the repetitive parts of their job to scale their impact and focus on high value investigation. They want to codify their standard operating procedures into workflows that can automatically enrich alerts, gather context during an incident, and handle low level triage, freeing them from manual toil.
Secondary Persona: The Team Lead / Architect
* Who they are: SOC Managers, Observability Leads, and Platform Architects.
* What they do: They are responsible for the overall health, efficiency, and strategy of their operational domain. They design processes, manage team workload, and ensure the right tools and procedures are in place.
* What they need: They need to build durable, reusable, and manageable automation that the entire team can rely on. They will use One Workflow to standardize response processes, ensure compliance, and create a library of pre-approved automations that practitioners can use safely. They are focused on reliability, governance, and proving the value of automation.
Tertiary Persona: The Developer / Builder
* Who they are: Developers building custom applications with Elastic Search, or internal tool builders creating bespoke solutions for their organization.
* What they do: They use Elastic as a backend to power search experiences, business analytics applications, and other custom software.
* What they need: They need a programmatic way to embed complex, multi-step business logic into their applications without building an orchestration engine from scratch. They will use the One Workflow API to trigger automations for tasks like user onboarding sequences, complex data processing pipelines, or backend business process management.
Detailed User Stories
This document captures the detailed, foundational user stories for the unified workflow automation capability. These stories are outcome-focused and are intended to provide a clear "why" for the features outlined in the product requirements.
Workflow Definition & Creation
* As a user, I want to define a sequence of steps in a workflow, so that I can automate a multi-stage process from start to finish.
* As a user, I want to design and modify workflows on a visual, drag-and-drop canvas, so that I can easily understand, communicate, and debug the flow of complex logic without having to read code.
* As a user, I want to define my workflows declaratively as code (YAML), so that I can version control them, review changes, and integrate them into our GitOps practices.
* As a user, I want to get started quickly by using a pre-built template from a gallery, so that I can solve a common problem without starting from scratch.
* As a user, I want to generate a functional workflow from a plain-text description, so that I can leverage AI to accelerate the initial creation process.
* As a user, I want to build modular and reusable automation by nesting one workflow inside another (sub-workflows), so that I can avoid rebuilding common logic and create more maintainable processes.
Workflow Triggers & Execution
* As a user, I want to define specific triggers for my workflows, so that they can start automatically when certain conditions are met, whether on a schedule or in response to an event.
* As a user, I want to trigger a workflow from an external system via a secure webhook, so that I can integrate our automation with any application that can send an HTTP request.
* As a user, I want to start a workflow based on an event happening within the Elastic ecosystem (like a new security detection), so that I can create tightly integrated, end-to-end automations.
* As a user, I want to run a workflow on a recurring schedule, so that I can automate routine maintenance, reporting, and monitoring tasks.
* As a user, I want to manually execute a workflow on-demand, so that I can test its logic during development or run tasks that require human initiation.
Flow Control & Logic
* As a user, I want to add conditional logic (if/else branches) to my workflows, so that my automations can adapt their behavior based on different inputs or the results of previous steps.
* As a user, I want to execute multiple branches of a workflow in parallel, so that I can perform independent tasks concurrently and reduce the total execution time.
* As a user, I want to wait for all parallel branches to complete before proceeding, so that I can safely orchestrate complex processes that have multiple independent dependencies.
* As a user, I want to repeat a set of actions for every item in a list, so that I can process batch data efficiently.
* As a user, I want to define robust error handling paths, so that I can control what happens when a step fails and prevent the entire workflow from crashing.
* As a user, I want to configure automatic retries and timeouts for individual steps, so that I can build resilient workflows that can recover from transient failures.
Data Handling & Transformation
* As a user, I want to easily access and pass data between steps in a workflow, so that I can build a coherent process where each step can use the outputs of those before it.
* As a user, I want to have a set of built-in tools to transform data, such as parsing JSON, manipulating strings, and formatting dates, so that I can ensure data is in the correct format for each step.
* As a user, I want to execute my own custom scripts (e.g., Python, JavaScript), so that I can perform complex data transformations or business logic that isn't available out-of-the-box.
* As a user, I want to use AI to enrich or reformat my data using natural language instructions, so that I can automate complex data mapping and manipulation tasks without writing code.
Connectors & Actions
* As a user, I want to connect to our external tools and services from a central place, so that I can manage all our integrations and credentials securely and efficiently.
* As a user, I want to perform actions in other applications as part of my workflow, so that I can orchestrate processes that span the entire toolchain of my organization.
* As a user, I want to create a "human-in-the-loop" step that pauses a workflow and waits for manual approval, so that I can automate processes that still require human judgment or authorization.
Workflow Management & Governance
* As a user, I want to see a full version history for my workflows, so that I can track changes over time, understand what was altered, and revert to a previous version if needed.
* As a user, I want to view a detailed audit trail and logs for every workflow execution, so that I can troubleshoot failures and prove compliance.
* As a user, I want to have clear, role-based access controls for workflows and connections, so that I can ensure that only authorized team members can view, edit, or execute automations.
* As a user, I want to synchronize our workflows with a Git repository, so that I can manage our automation assets with the same rigor and CI/CD processes as our application code.






Core Components
Core Components of Workflow Automation
To understand how Elastic will deliver workflow automation through One Workflow, we need to break it down into its core components. This section defines the essential building blocks that make workflow automation possible: how workflows are structured, triggered, executed, and extended.
It also establishes a shared vocabulary across our team. By giving each concept a clear definition and role, we can align on how we reason about the system, how we design for it, and how we build around it.
  

Different platforms use wildly different terms, such as Zap, Scenario, Story, Process, Flow, Workflow, and Recipe, to describe what are often the same underlying concepts. It’s funny, but also a real PITA when it comes to user research, competitive analysis, or internal planning.
Here are the key components we’ll cover (keep in mind these aren’t requirements, but at a high level, capture the definitions that make up workflow automation):
  

* Workflow Triggers: The events or conditions that initiate a workflow. 
* Workflow Steps: The individual units of logic or action within a workflow. Steps define how data moves, decisions are made, and results are produced.
* Connectors: The interface between Elastic and external systems. Connectors enable workflows to act on or respond to external events and services.
* Workflow: A structured, reusable sequence of triggers and steps designed to achieve a specific outcome through automation.
Each section above explains the role of these components, how they work together, and why they matter. Once we align on their definitions and high-level details, we can begin defining their detailed requirements.   






Triggers
Workflow Triggers
What is a Trigger
A trigger is any event or condition that initiates the execution of a workflow. It defines the moment a workflow comes to life. Without a trigger, workflows remain dormant definitions. Triggers give them relevance by connecting them to real-world signals, schedules, or user actions.


  



Triggers enable a workflow automation tool to respond immediately to changing conditions, emerging signals, or explicit requests.


Triggers also seed workflows with initial context. This originating data provides meaning to the workflow run. For example, a workflow triggered by an alert might carry with it the alert’s metadata, entities, and source events. This context shapes how the workflow executes.
Types of Triggers
Time-Based Triggers
These initiate workflows based on time schedules. They are useful for recurring maintenance tasks, scheduled checks, periodic data syncs, or any other task that needs to run at fixed intervals.
Event-Based Triggers
These are initiated by a specific signal, condition, or state change. Events can come from inside Elastic (like a detection rule) or from external systems (like an Okta user creation event or a GitHub webhook). The event itself can be used as context throughout the execution of the workflow
Manual and Chat-Initiated Triggers
Some workflows are triggered directly by a user. This could be through a button in the UI, a command issued in a chatbot, or an API call from another system. This category covers both direct interaction and conversational interfaces.


Examples:
* An analyst clicks “Start Investigation” on a case timeline. 
* A user types “summarize recent alerts for host X” in a Slack assistant, which triggers a workflow.


These workflows often serve as interactive tools, letting users initiate automation without leaving their current context.
Workflow-Initiated Triggers
Workflows can also trigger other workflows. This composability enables complex systems to be broken into smaller, modular parts. One workflow might gather data and, based on the results, trigger specialized downstream workflows for enrichment, triage, or notification.


Examples:
* A parent workflow that runs hourly asset discovery triggers a follow-up workflow for each new asset to assess exposure.
* An attack discovery workflow triggers a validation workflow to investigate related activity across entities.


This approach supports reusability, clearer logic, and decoupling of responsibilities.




Steps
Workflow Steps
What are Workflow Steps
Workflow steps are the fundamental building blocks of automation. Each step represents a single unit of logic, action, transformation, or reasoning. Together, they define how a workflow operates and what outcomes it can produce. Steps are chained together to move data, coordinate logic, and drive results.


Without steps, workflows are just plans. Steps are what turn intent into execution.


  

Step Categories
Workflow steps can be grouped into four main categories, based on their function within the automation.
Flow Steps
Flow steps shape how a workflow moves. They control the order, structure, and branching logic of execution. This includes:
* Conditional logic: Execute certain steps only when conditions are met
* Pauses and waits: Introduce delays or time-based holds
* Early exits: Skip or halt execution when needed
* Custom scripting: Define custom logic that governs how the workflow progresses based on complex or non-standard conditions


These steps make workflows dynamic and responsive. They allow workflows to adapt in real time to data and conditions.
Action Steps
Action steps carry out operations in internal or external systems. These are the steps that allow workflows to have real-world impact.
* Interact with Elastic features across solutions. This includes common operations like:
   * Querying data from Elasticsearch or data streams
   * Indexing new documents or updating existing fields
   * Closing or updating cases
   * Enriching alerts with additional context
   * Modifying dashboards or saved objects
* Trigger actions in external systems via APIs, integrations, or service connectors
* Send messages, alerts, or notifications to systems like Slack or email
* Invoke other workflows


Within Elastic, these actions are exposed through a simple abstraction. Users do not need to know specific API endpoints or internal implementation details. Instead, we provide a curated and extensible set of platform operations that are safe, discoverable, and aligned with how users already think about Elastic’s capabilities.
Transform Steps
Transform steps prepare and reshape data as it flows through a workflow. They allow workflows to interpret, modify, and organize information.
* Add or remove fields
* Convert formats or types
* Combine or split values
* Loop through an array of data
* Custom scripting: Transform data in custom or complex ways when standard operations are not sufficient


Transformation is often needed between steps to ensure downstream actions have the right input.
AI Steps
AI steps introduce reasoning and language understanding into workflows. These steps allow workflows to process natural language, make context-aware decisions, or operate through agents.
* Summarize or interpret information using a language model
* Extract key insights from unstructured data
* Implement an agentic step where an AI agent is configured with a goal, access to tools, and autonomy to act


Users can define the instructions or prompt that guides the agent, the tools it has access to, and the outcome it is expected to achieve. They can also specify which large language model powers the step. This creates a powerful abstraction that combines automation with adaptive intelligence.






Connectors
Connectors
What are Connectors 
Connectors are the mechanism through which Elastic workflows integrate with external systems. They serve as the bridge that enables workflows to both react to external events and perform actions in external environments.
  

Connectors should be understood as the interfaces that extend Elastic’s workflow automation capabilities beyond the platform itself. They allow workflows to:
* Be triggered by events or conditions originating outside Elastic
* Execute actions or send data to external systems and services


The final naming and implementation of this concept is something we need to discuss in depth. This concept isn’t new; it overlaps with existing constructs such as Kibana connectors, search connectors, and integrations (and now, of course, keep providers). These systems form a sort of Venn diagram of capabilities, and careful consideration will be needed to avoid fragmentation or confusion as we build out the e2e user experience.  
Why Connectors Matter
Most workflows do not operate in isolation. In real-world use cases, automation must interact with systems outside of Elastic. Whether a workflow needs to respond to external event third-party system or update a ticket in an external incident management platform, connectors are what make that possible.


Connectors are what allow workflows to:
* Extend their reach to external tools and data sources
* Become part of broader enterprise processes and environments
* Act as a bridge between Elastic-native insights and external actions


Without connectors, the utility of workflow automation is limited to what happens within Elastic.






Workflow
Workflow
What is a Workflow
A workflow is a defined sequence of steps designed to achieve a specific outcome through automation. It provides a structured way to execute logic, coordinate steps, and carry context across systems.
  

How Workflows Are Structured and Executed
Workflows in Elastic are designed to be durable. They maintain state throughout their lifecycle. They can pause and resume, recover from failure, and retain the data and execution path needed to understand what happened and why. This persistence is especially important for long-running or multi-stage processes.


Workflows are also composable. Complex processes can be broken down into smaller, reusable units. One workflow can trigger another. This allows teams to create modular systems that evolve over time without becoming fragile or monolithic.


Workflows are defined declaratively in YAML and can be managed as code, not just through Kibana. They are versionable and CI/CD friendly. The fact that workflows are defined in YAML opens the door for natural language to be used in workflow definition via our assistant, which will significantly improve accessibility for users while accelerating the experience of experienced teams.


Elastic's workflow engine brings these capabilities to life. It handles execution, tracks state, manages retries, supports scheduling, and provides the necessary monitoring to understand workflow behavior as it occurs.
Who Creates and Uses Workflows?
Workflows are a flexible building block, designed to be authored by users and embedded within Elastic features alike.


Users can create workflows to automate their own tasks and operations, either through a visual builder or by defining them as code. These workflows can capture repeatable processes and automate cross-system coordination.


Elastic features can also define and trigger workflows. For example, the Attack Discovery feature in Elastic Security analyzes open alerts using an LLM to identify potentially coordinated attacks. When a discovery is made, it can trigger a registered workflow that automates the follow-up investigation. This includes identifying involved entities, analyzing related events across the environment, and validating whether the attack is a true positive. The result is increased trust in the detection and reduced manual effort for the analyst.[a]
Types of Workflows
Supporting Concepts (will expand on PRD)
Human-in-the-Loop
Some workflows include human checkpoints. These might be approval steps, manual reviews, or judgment calls. Instead of breaking the automation, these steps are modeled as part of the workflow. The system waits at these checkpoints until input is received, then continues. This ensures workflows can span machine-driven logic and human decision-making without friction. 
Resumable Workflows
Workflows can be paused and resumed. This is essential for long-running processes or steps that require human input or depend on external systems. Elastic's workflow engine tracks execution state so workflows can pick up where they left off. This makes automation more robust, especially across interruptions.
Workflow Dependencies
Workflows can depend on one another. For example, an enrichment workflow may need to complete before a triage workflow can proceed. These dependencies can be defined explicitly. They support orchestration of complex systems without forcing all logic into a single flow. Teams can build independent, reusable workflows that interact cleanly.
Workflow Execution Context
The workflow execution context is the structured information carried alongside a workflow during its execution. It includes metadata such as the workflow ID, execution timestamp, origin of the trigger, input parameters, intermediate data, and outputs from previous steps. This context provides the necessary state for steps to make decisions, pass data, and take action with continuity and relevance.


It ensures each workflow instance is self-contained, traceable, and capable of resuming if interrupted. Context enables branching logic, debugging, observability, and state awareness as workflows progress. It will also be key during the development of workflows, as users refine and build them iteratively by re-emitting data from previous steps until they get their workflow definition right.
Workflow Templates
Workflow templates are prebuilt, reusable workflow definitions that help users get started quickly with automation. These templates can be used as-is or customized to fit specific use cases. Elastic will provide a curated set of out-of-the-box templates that are available publicly and, ideally, discoverable directly within the product experience. In the future, we envision the community playing a key role in expanding the template ecosystem: contributing domain-specific automation patterns, sharing operational best practices, and enabling a broader range of use cases than any single team could anticipate. This communal approach helps lower the barrier to entry while also advancing the state of automation across Elastic’s user base.
Deterministic Workflows
Deterministic workflows follow a fixed logic. Given the same input, they will always produce the same output. These workflows are ideal for well-understood, repeatable tasks like provisioning users, rotating credentials, or archiving logs. They are reliable, easy to test, and form the foundation of most process automation.
Agentic Workflows
Agentic workflows incorporate steps powered by large language models. These steps allow a workflow to interpret context, reason about ambiguous inputs, and select from multiple potential actions. Agentic workflows are suited for situations where fixed logic is not enough and human-like decision-making is required.






Use Cases & Examples
Use Cases: Background
This section outlines the primary use cases and examples for our workflow automation capability. A critical point must be understood as we review these examples: we are not building distinct, point-in-time solutions like a standalone SOAR or AIOps product.


Instead, we are building a single, robust, and extensible workflow automation platform that is a core component of Elastic.


The solution-specific use cases discussed below (Security Orchestration, Automation, and Response (SOAR) in Security, AIOps in Observability, and business process automation (BPM) in Search) are the crucial outcomes this foundational capability will enable. Our primary focus is on delivering a world-class automation platform; this will, in turn, allow us to deliver these curated and powerful experiences to the specific personas who rely on our solutions.


Examples
This section provides concrete examples of the outcomes our workflow automation capability will deliver. These use cases should be viewed as powerful applications of a single, unified automation engine, not as distinct, standalone products. Our primary focus is on engineering this core capability, which then enables us to deliver tailored outcomes for:
* Security: Enabling Security Orchestration, Automation, and Response (SOAR) to accelerate threat detection and response 
* Observability: Delivering AIOps capabilities for proactive incident management and automated remediation 
* Search: Powering business process automation (BPM) for custom data-driven application and or general IT automation 
Building on our user stories, the following examples illustrate these use cases with concrete examples.
Security (SOAR)
Use Case: Automated Alert Triage & Enrichment
* Goal: Automatically enrich incoming alerts with internal and external context to reduce manual investigation time and allow analysts to focus only on high fidelity threats.
* Example Workflow: Malware Alert on Host
   1. Trigger: A new detection alert fires for "Malware Detected on a Host."
   2. Extract Entities: The workflow parses the alert to extract key entities like the hostname, the process name, and the file hash.
   3. Internal Enrichment:
      * It queries an internal entity store index to get the host's owner and business criticality.
   4. External Enrichment:
      * It sends the file hash to an external threat intelligence provider/aggregator like VirusTotal.
      * It checks the reputation of the process name against a threat feed.
   5. Conditional Logic:
      * If the file hash is known malicious:
         * The workflow updates the alert severity to Critical.
         * It creates a new case associated with the alert.
         * It adds a comment to the new case with a summary of all enrichment data.
         * It sends a high priority notification to the SOC Slack channel with a link to the case.
      * Else If the file hash is unknown:
         * The workflow creates a new Medium severity case.
         * It assigns the case for analyst review.
      * Else:
         * The workflow closes the alert as a false positive.
Use Case: Proactive Threat Hunting & IOC Sweeping
* Goal: When a new high-confidence Indicator of Compromise (IOC) is identified, automatically sweep the entire environment to find any evidence of its presence, past or present.
* Example Workflow: New IOC Identified
   1. Trigger: Manual trigger by a threat hunter, providing an IOC (e.g., a file hash, domain name, or IP address) as input.
   2. Parallel Search: The workflow initiates several searches in parallel across different data sources:
      * Search all endpoint security data for the file hash.
      * Search all DNS and proxy logs for the domain name.
      * Search all firewall and network flow logs for the IP address.
   3. Wait for Completion: The workflow pauses until all searches have completed.
   4. Aggregate & Analyze: It aggregates the results from all parallel searches into a single list.
   5. Conditional Logic & Response:
      * If any hits are found:
         * Create a new case with a title like "IOC Sweep Hit Found: [IOC Value]".
         * Populate the case with a timeline of all sightings of the IOC, including affected hosts and users.
         * Add the IOC to a high-priority blocklist.
         * Assign the case to the incident response team for immediate investigation.
         * Send a Slack message to the incident response channel with a link to the case and a summary of the findings.
      * Else (no hits found):
         * Log the completion of the sweep with a "no results" status for audit purposes.
         * Add the IOC to a watch-list for future monitoring.
Use Case: Automated Secret Leak Response
* Goal: Instantly respond to a notification of a leaked secret by revoking the credential and notifying the responsible team.
* Example Workflow: Leaked Credential Detected in GitHub
   1. Trigger: An inbound webhook is received from GitHub's secret scanning service.
   2. Extract Information: The workflow parses the webhook payload to extract the leaked secret, the repository name, and a link to the commit.
   3. Identify Secret Type: A step analyzes the secret's format to identify it (e.g., an AWS Access Key, a Google Cloud service account key, a database connection string).
   4. Automated Revocation:
      * If the secret is an AWS Key, the workflow uses an AWS connector to immediately disable the key.
      * If the secret is a Google Cloud key, it calls a GCP connector to revoke the service account key.
   5. Identify Ownership: The workflow queries an internal entity store to find the team that owns the GitHub repository.
   6. Create Case & Notify:
      * It creates a new Critical severity case to track the incident.
      * It sends a detailed notification to both the responsible team's Slack channel and the central security incidents channel. The message includes: "Leaked AWS Key found in repository [repo_name]. The key has been automatically revoked. Please rotate credentials immediately. Case: [link to case]".
Use Case: Automated S3 Bucket Exposure Response
* Goal: Automatically detect and remediate publicly exposed S3 buckets based on their data classification.
* Example Workflow: Public S3 Bucket Detected
   1. Trigger: A CSPM finding is generated for a public S3 Bucket 
   2. Extract Information: The workflow parses the alert to get the name of the S3 bucket and the relevant cloud account ID.
   3. Internal Enrichment:
      * The workflow queries the internal entity store for the bucket name.
      * It retrieves the bucket's data classification tag (e.g., Confidential, Internal, Public) and the owning team tag.
   4. Conditional Logic:
      * If the data classification is Confidential:
         * Remediate: The workflow immediately uses an AWS connector to set the bucket's public access block to true.
         * Create Case: It creates a Critical severity case with a title "CRITICAL: Public S3 Bucket with Confidential Data Remediated."
         * Notify: It sends a high-priority Slack message to both the incident response team and the owning team, informing them of the exposure and the automatic remediation.
      * Else If the data classification is Internal or not specified:
         * Create Case: It creates a Medium severity case and assigns it to the owning team for manual review.
         * Notify: It sends a Slack message to the owning team's channel with the details of the public bucket and a link to the case, asking them to review and remediate within 24 hours. A notification is also sent to a central security operations channel for visibility.
      * Else (if data classification is Public):
         * The workflow closes the alert as benign, adding a note that the public status is intentional.
Observability (AIOps)
Use Case: Automated Root Cause Analysis
* Goal: When a service outage occurs, automatically gather diagnostic information to reduce the mean time to resolution (MTTR).
* Example Workflow: APM Service Degradation
   1. Trigger: An APM alert fires for "High Latency Detected in checkout-service."
   2. Gather Context:
      * The workflow queries for related logs and traces from the checkout-service within the incident timeframe.
      * It calls a connector to the CI/CD system (e.g., Jenkins, GitHub Actions) to check for recent deployments to that service.
      * It queries infrastructure metrics (CPU, Memory, Disk I/O) for the underlying pods or VMs.
   3. Correlate & Analyze: A language model step summarizes the anomalous logs and correlates the timing of the degradation with any recent deployments.
   4. Create Incident Record: The workflow automatically opens a new incident ticket in ServiceNow or Jira.
   5. Populate & Notify:
      * It populates the incident ticket with the summary of findings, links to relevant logs and traces, and details of the recent deployment.
      * It pages the on-call SRE for the checkout-service via PagerDuty, including a link to the populated ticket.
Use Case: Proactive Remediation
* Goal: Detect conditions that are known precursors to an outage and take automated action to prevent it.
* Example Workflow: Impending Disk Full
   1. Trigger: A metric threshold alert fires when the disk usage percentage on a database node exceeds 90%.
   2. Initial Action: The workflow executes a built-in step to run a cleanup script that purges temporary files and old log archives on the affected node.
   3. Wait & Verify: It pauses for 5 minutes.
   4. Conditional Logic:
      * It re-checks the disk usage percentage metric.
      * If the metric is now below 85%:
         * The workflow resolves the initial alert and logs its successful action.
      * Else:
         * The issue was not resolved by the initial action. The workflow escalates by creating a high priority incident ticket for the database team to investigate manually.
Search
To do…




Requirements
Introduction 
This section outlines the product requirements for introducing workflow automation capabilities within Elastic. The goal is to build a powerful, enterprise-grade workflow automation platform that delivers on the key use cases we envision for our security, observability, and search solutions. 


Introduction
General Workflow Requirements
Workflow Creation
Workflow Definition
Workflow Management
Workflow Template Gallery
Workflow Triggers
General Trigger Requirements
Webhook Triggers
App Event Triggers
Scheduled & Polling Triggers
Manual Triggers
Workflow Steps
General Step Requirements
Action Steps
Flow Control Steps
Data Transformation Steps
AI Steps
Connection & Credential Management
Identity & Access Control
Solution Integration & Experience
Non-Functional Requirements
Scalability & Performance
Usability
Security
Deployment & Supportablity


General Workflow Requirements
This Section covers requirements for the general definition, execution model, and lifecycle management of workflows.
Workflow Creation
This subsection outlines the different methods users have to create a new workflow[b].


Requirement Category 
	Users must be able to… 
	Workflow Creation: Blank
	...create a new, blank workflow to begin the authoring process.
	Workflow Creation: From Clone
	…clone an existing workflow, including its full definition and configuration, to create a new, editable copy.
	Workflow Creation: From Template 
	...create a new workflow by using a pre-built, cloneable template as a starting point.
	Workflow Creation: AI-Assisted
	...generate a workflow from a plain text description using an AI assistant.
	Workflow Creation: From Import 
	...import a workflow definition file to create or update a workflow within the platform.
[c]	
Workflow Definition
This subsection outlines the capabilities users have to define the logic and behavior of a workflow.


Requirement Category 
	Users must be able to… 
	Workflow Definition: Identification
	...assign a name[d] and an optional description to a workflow for easy identification.
	Workflow Definition: Triggers
	...define one or more triggers[e] that can initiate the workflow.
	Workflow Definition: Steps 
	...define the workflow's logic by adding a sequence of one or more steps
	Workflow Definition: Visual 
	...visually compose workflows by connecting triggers and steps on an intuitive, drag-and-drop canvas.
	Workflow Definition: Declarative 
	...define workflows declaratively using a structured YAML format.


We must provide a versioned YAML schema for declarative workflow definitions and ensure backward compatibility for workflows written against older schema versions.
	Workflow Definition: View Switching
	...switch seamlessly between the visual and declarative views of a workflow at any time. 


We must ensure that the visual canvas and the declarative YAML editor are always in sync and have full feature parity.
	Workflow Definition: Annotations 
	...add notes or descriptions directly onto the workflow canvas to document complex logic or provide context.
	Workflow Definition: Modularity
	...create reusable components by nesting one workflow within another (sub-workflows).
	Workflow Definition: Development Execution / Debugging [f] 
	...manually execute a workflow, either a specific step or the whole workflow,  on-demand to test its logic and iterate during development, without needing to fully enable it 
	Workflow Definition: Development Execution / Debugging 
	…visually inspect the data inputs and outputs of each step in real-time during a manual test execution.
	Parallel Execution
	...define multiple, independent branches of steps that originate from a single point in the workflow. 


… we should view this as a requirement to enable the execution of multiple branches concurrently, with the user having full control over the logic within each branch, including whether a branch proceeds to a common subsequent step or terminates independently.
	Workflow Management
This subsection outlines the capabilities users have to manage & monitor their workflows after they have been created and defined.


Requirement Category 
	Users must be able to… 
	Workflow Management: Discovery
	...view a list of all workflows they have access to.
	Workflow Management: Organization
	...organize workflows into logical groups, such as with folders or tags, for easier navigation.
	Workflow Management: Export 
	...export a workflow by downloading or copying its declarative YAML definition.
	Workflow Management: Metadata
	...view key metadata for a workflow, such as its owner, creation date, and last modified date.
	Workflow Management: Version Control
	...track different versions of a workflow, view change history, compare any two versions side-by-side to highlight changes, and revert to any previous version.
	Workflow Management: Enable Workflow
	...activate a workflow, allowing it to be executed when its trigger conditions are met.
	Workflow Management: Disable Workflow
	...deactivate a workflow, preventing it from being executed without deleting it.
	Workflow Management: Delete Workflow
	...permanently delete a workflow from the platform.
	Workflow Management: Execution History[g]
	...review a detailed, searchable audit trail for every workflow run, including the status, duration, and data for each step. [h]
	Workflow Management: Monitoring




	...view the health and performance of all workflows (e.g., success/failure rates, execution counts).
	Workflow Management: As Code 
	…synchronize workflows between Elastic and a version control system, enabling workflows-as-code practices.[i][j]
	Workflow Template Library 
This section covers requirements for a centralized library where users can discover and utilize pre-built workflow templates.[k][l]




Requirement Category 
	Users must be able to… 
	Template Discovery & Browsing
	…access a library to browse all available out-of-the-box workflow templates. This interface must support searching by name or keyword and filtering by use case (e.g., Security, Observability, IT Operations) or by application (e.g., Jira, Slack).
	Template Details View
	When a user selects a template, they must be able to view its full definition, a description of what it does, the required connections, and any configuration notes before choosing to use it.
	Community Contributions
	We must provide a mechanism for users to submit their own custom workflows to be shared as templates with the broader Elastic community
	To do [m]
	

	Workflow Triggers 
This section details the various types of triggers that can be configured to initiate a workflow's execution.
General Trigger Requirements
These requirements apply to all trigger types configured for a workflow.


Requirement Category 
	Users must be able to… 
	Identification[n]
	...assign a custom, descriptive name to a trigger for easy identification.
	Output
	...access the data and metadata from the triggering event as the initial context for the workflow.
	Conditions
	...define conditions[o] on a trigger to filter events, preventing the workflow from running unless the data from the trigger meets the specified criteria.
	UI-Based Selection




	… select a trigger from a menu that is clearly organized by category when using the visual canvas/workflow builder (e.g., Webhook, App Events, Scheduled, Manual)  
	Webhook Triggers
This subsection covers triggers that are initiated via an HTTP request, a common mechanism for receiving events.


Requirement Category 
	Users must be able to… 
	Invocation
	...initiate a workflow by sending an HTTP request to a unique URL provided by the platform.
	Configuration 
	… configure different attributes about a webhook, like headers. [p]
	Security
	...secure a webhook trigger, for example, by requiring an API key or a signed payload, cert, etc., [q]
	App Event Triggers
This subsection covers triggers that respond to a discrete event from either an “interna”l or external system/application. Keep in mind that while all triggers are technically event-driven, this category refers to events (from the user's perspective) that can be concretely defined, as opposed to scheduled or manual triggers.


Requirement Category 
	Users must be able to… 
	Internal Event
	...initiate a workflow by selecting from a list of available event types originating from within Elastic (e.g., a detection rule firing in Security, a new case being created, etc.,)
	External App Event
	...initiate a workflow by first selecting a connected third-party application, and then choosing from a list of available trigger events for that specific app (e.g., 'new ticket created' in Jira).
	Scheduled & Polling Triggers
This subsection covers triggers that run based on time or periodic checks.


Requirement Category 
	Users must be able to… 
	Scheduled
	...initiate a workflow at a specific time and on a recurring basis (e.g., cron schedule).
	Polling 
	...initiate a workflow by having the platform periodically check an external system or internal Elastic data for new or updated information (e.g., running a query against an index for new documents that match a specific criteria).
	Manual Triggers 
This subsection covers triggers that are initiated on-demand, whether directly by a user, the AI Assistant, or programmatically via an API.




Requirement Category 
	Users must be able to… 
	UI-Based
	...manually execute a workflow by clicking a "run" button in the user interface.[r]
	AI Assistant-Based
	...initiate a workflow by invoking it directly from the Elastic AI Assistant in Search, o11y, and Security 
	API-Based
	...programmatically execute a workflow via a public API endpoint.[s]
	Workflow Steps
This section details the requirements for workflow steps (categories of functional building blocks) that users can add to a workflow to define its logic/operation. 
General Step Requirements
These are capabilities that apply to all types of steps within a workflow.


Requirement Category 
	Users must be able to… 
	Step Identification
	...assign a descriptive name to each step for easy reference and to improve workflow readability.
	Data Context
	...easily reference outputs from any prior step as inputs in subsequent steps within the workflow's data context.[t]
	Failure Paths
	...define an alternative logic path to execute if a specific step fails.
	Step-Level Retries
	...configure automatic retry policies on applicable steps (such as actions or scripts) to handle transient failures.
	Debugging 
	…re-execute a and have it re-emit the same event payload 
	Action Steps [u]
These steps perform a specific action, typically by interacting with an internal or external system.


Requirement Category 
	Users must be able to… 
	Internal Actions
	...select an action from a list of available actions that interact with the Elastic ecosystem (e.g., "Create a Case," "Elasticsearch query”, etc.,)
	External App Actions
	...select an action by first choosing a connected third-party application, and then choosing from a list of available actions for that app (e.g., "Create Jira Ticket").
	Generic HTTP Request
	...make a generic API call by configuring all aspects of an HTTP request, including the method (e.g., GET, POST, PUT), URL, headers, query parameters, and request body.
	Flow Control Steps
These steps are used to direct the path of execution within a workflow.




Requirement Category 
	Users must be able to… 
	Conditional (If/Else) Logic
	...direct the workflow down different paths based on a user-defined condition that evaluates to true or false (e.g., if an alert's severity is 'critical', page the on-call; else, create a ticket).[v][w]
	Conditional (Switch) Logic
	...direct the workflow down one of several paths based on the value of a user-selected data point (e.g., based on the alert source, route it to the correct team's Slack channel).
	Looping / Iteration
	...repeat a set of actions for each individual item in a list or collection (e.g., process each IP address in a list returned from a threat intel service).[x]
	Throttling
	...add a dedicated throttle step that takes a batch of items as input and emits them at a consistent, user-defined rate by specifying the number of items per run and the number of runs per time interval.[y]
	Wait / Delay
	...pause the execution of the workflow for a user-defined duration (e.g., 5 minutes, 2 hours).
	Data Transformation Steps
These steps are used to manipulate, process, or persist data within a workflow. [z]


Requirement Category 
	Users must be able to… 
	Field Manipulation: Edit Fields 
	…add new fields, remove existing fields, or modify the values of fields (ex., TO UPPER and TO LOWER) within a data item 
	Field Manipulation: Date/Time
	… format, modify, and perform calculations on date and time values
	List processing: Filter 
	…remove items from a list based on a user-defined condition 
	List processing: Remove Duplicates 
	…remove duplicate items from a list based on the value of a specific field 
	List Processing: Split
	...separate a single data item containing a list into multiple, individual items.
	Data Processing: Merge / Join 
	...combine two or more separate lists (arrays) of data from the workflow context into a single, unified list. 


For example, in the case of parallel branch execution in a single workflow, this can be used to define a join point that waits for all active parallel branches to complete before allowing a workflow to proceed.
	List Processing: Aggregate / Append 
	...combine multiple individual items into a single item containing a list of those items.
	List Processing: Summarize
	...perform aggregate calculations (e.g., SUM, COUNT, AVG, MAX, MIN) across a list of items.
	Custom Scripting
	...execute custom scripts (e.g., Python, JavaScript) in a dedicated "code" action for complex transformations.
	AI Transformation
	...transform or enrich data by providing natural language instructions to an AI model.
	AI Steps 
These steps leverage an LLM to generate content or orchestrate tasks 


Requirement Category 
	Users must be able to… 
	Generic AI step
	…add an AI step that calls an LLM by selecting a model, providing a set of custom instructions (prompt), and defining the desired output 
	Agentic Step
	…add a step that calls an LLM and provides it with a specific goal, custom instructions, and explicit access to a limited set of actions (tools) that the LLM can use to achieve the goal defined 
	Connection & Credential Management 
This section covers requirements for how users configure, authenticate, and manage connections to third-party applications and services. 


Requirement Category 
	Users must be able to… 
	Connector Configuration
	…add and configure connections to external applications 
	Authentication Methods 
	… securely provide credentials for these connectors using various authentication methods, including API keys/tokens 
	Expiration Date 
	…optionally define an expiration date for a credential [aa]
	Connection Health
	…test a connection’s status to validate that it is active and correctly configured. 


We should proactively indicate when a connection is broken. 
	Credential Security
	We must ensure all sensitive credential information (ex., API keys, secrets) is encrypted at rest and never exposed in plaintext in the UI or logs after initial entry 
	Identity & Access Control
This section covers requirements for RBAC, user permissions, and resource sharing within teams and organizations 


Requirement Category 
	We must…
	User Roles
	Provide a set of predefined user roles (e.g., Administrator, Editor, Viewer) that determine a user’s default permissions. 
	Workflow Permissions 
	Provide granular control over which roles can create, view, edit, run, enable/disable, and delete workflows 
	Connector Permissions 
[ab]	
Provide control over which roles can create, view, edit, use, and delete shared connections to external applications/services. 
	Secure Sharing 
	Provide a mechanism for users to securely share workflows and their associated connections with other users or teams within their organization, respecting the defined permission models.
	Solution Integration & Experience 
This section outlines how workflow automation capabilities are surfaced across our Search, Observability, and Security solutions in order to provide a context-aware user experience. 


Requirement Category 
	

	Context-Aware UI
	When users interact with workflow automation from within a specific solution (e.g., Elastic Security), the UI must be tailored to that context. This includes prioritizing the display of relevant workflow templates, triggers, and actions (e.g., surfacing SecOps templates and security-related actions first), while still allowing access to the full set of capabilities.
	Embedded Workflow Triggers
	Users must be able to configure and trigger a workflow directly from within the UI of specific features/capabilities across our solutions, where actions are taken. For example, when defining the actions for a detection rule in Elastic Security, “Run a Workflow” (final text tbd), must be available as a native action type, allowing users to select an existing workflow to execute when the rule’s conditions are met. 
	Non-Functional Requirements
This section defines the essential quality attributes the platform must exhibit.
Scalability & Performance
This section outlines the requirements for the platform's ability to handle load and perform efficiently. 


Requirement Category 
	We must…
	Dynamic Scalability 
	…design the platform to dynamically increase and decrease its capacity to handle sudden bursts of high-volume workflow executions. 
	Execution Throughput 
	…design the platform to handle event-driven storms, with the goal of supporting tens of thousands of concurrent workflow executions
	Trigger Latency 
	…ensure that latency-sensitive triggers, such as webhooks, initiate workflows within a defined low-latency threshold (TBD) 
	UI Responsiveness 
	… provide a fluid and responsive visual authoring experience, even when displaying and editing large or complex workflows 
	Usability 
This section details the requirements for making the platform easy to learn, use, and build upon.


Requirement Category 
	We must…
	Comprehensive Documentation
	…provide clear, accurate, and easily searchable documentation for all features, including tutorials, examples, and API references.
	Actionable Error Messaging
	…ensure error messages in the UI and logs are clear, informative, and provide actionable guidance to help users diagnose and resolve issues efficiently.
	API-First Design
	…ensure all capabilities available through the user interface must also be programmatically accessible via a stable, well-documented public API.
	Security 
This section covers the requirements for protecting data within the platform and providing secure audit capabilities.


Requirement Category 
	We must…
	Data Encryption
	…ensure to encrypts all sensitive data (like creds) at rest and in transit
	Audit Logging
	…maintain a comprehensive and immutable audit log of all security-sensitive operations, such as changes to permissions, credentials, and workflows.
	Deployment & Supportablity 
This section defines the requirements for making the platform available across Elastic's various hosting models.


Requirement Category 
	

	Unified Availability [ac][ad][ae]
	Our workflow automation capabilities must be available across all primary Elastic deployment models: Elastic Cloud Serverless, Elastic Cloud Hosted (ECH), and ECE via Elastic Cloud on Kubernetes (ECK).
	Functional Parity 
	We must maintain core functional[af] parity across all deployment models, ensuring that users can build and execute the same workflows regardless of their chosen environment.
	Milestone Plan Overview
One Workflow - Milestone Plan Overview
Delivery Approach
Our milestone plan follows a progressive approach, delivering incremental value while building toward a complete workflow automation experience. Each milestone validates key assumptions, incorporates learnings, and expands capabilities based on user feedback.


Starting with foundational engine capabilities, we'll progressively add user experience enhancements, advanced features, and production-grade capabilities.


  

Milestone 1: Tech Preview (MVP)
Focus: Validate core workflow engine and execution model with developer-first approach. View the detailed milestone plan here. 
Key Deliverables:
* Core execution environment: Workflow engine running in Elastic Cloud Serverless with reliable, scalable execution
* YAML-based workflow definitions and editing: Declarative workflow authoring for technical users, enabling version control and GitOps practices
* Essential triggers: Rules, schedules, and manual execution to cover fundamental automation patterns
* Basic automation steps: Actions (Elastic and external system interactions), flow control (conditionals, loops), and data transforms
* Workflow management UI: Create, edit, and monitor workflow executions with essential debugging capabilities
Outcome: Technical validation that workflows can execute reliably within Elastic, with early adopters (developers, SREs, technical analysts) successfully creating foundational automations


Everything below this line is still being defined and is at a high level 
________________


Milestone 2: Beta
Focus: Democratize workflow creation and introduce intelligent assistance
Key Deliverables:
* Visual workflow builder: Drag-and-drop canvas with visual authoring, making workflow creation accessible to non-technical users
* AI-powered creation: Generate workflows from natural language descriptions, accelerating time-to-value
* Advanced debugging: Step-by-step execution and data inspection for complex workflow troubleshooting
* Team collaboration: Share workflows, manage permissions, and track usage 
* Expanded connector library: Broader integration ecosystem for common organizational tools
Outcome: Broader user base can create workflows without YAML expertise, validating the visual authoring experience and expanding use cases across security, observability, and business operations
Milestone 3: GA
Focus: Production readiness with comprehensive automation capabilities
Key Deliverables:
* Workflow library: Browse, customize, and use out-of-the-box templates for common patterns across security (SOAR), observability (AIOps), and business processes
* Advanced automation: Complex logic, error handling, ES|QL commands, and AI-powered steps for sophisticated use cases
* Complete audit capabilities: Full compliance and governance controls with immutable audit trails 
* Automation insights: Usage and execution analytics and health insights to optimize workflow performance and demonstrate business value
Outcome: Elastic is a trusted automation platform for organizations, enabling everything from simple task automation to complex AI-driven workflows with proper governance and scale
Beyond GA
Focus: Community-driven innovation and intelligent automation evolution
Key Deliverables:
* Community-driven library: User contributions, ratings, and collaborative workflow development fostering ecosystem growth
* Workflow recommendation: Learn from organizational patterns to suggest entirely new automation opportunities based on data and usage patterns
* Workflow migration: Enable users to automatically migrate existing workflows from other platforms (Tines, Splunk SOAR, etc.), preserving automation investments
* Advanced AI capabilities: Next-generation agentic workflows for autonomous operations
Outcome: Elastic becomes the center of gravity for workflow automation innovation, with a thriving community driving continuous advancement and organizations achieving autonomous operational intelligence
Continuous Innovation
Each milestone builds upon the previous, ensuring we deliver value early while maintaining a clear path to our ultimate vision: Elastic as the platform where data naturally flows into insight and insight seamlessly transforms into intelligent, automated action.








Milestone 1: Tech Preview (MVP)
Milestone 1: Tech Preview (MVP)
Objective
Validate and establish core workflow automation capabilities within Elastic by delivering a workflow engine that demonstrates reliable execution of declarative workflows.
Rationale
We're taking an engine-first approach to validate the core mechanics of workflow automation within Elastic's infrastructure. This foundation-first strategy is essential for building the right foundation for the future of automation:
  

By focusing on the foundational engine and YAML-based definitions first, we ensure reliable execution at scale before adding layers of complexity like visual builders. This approach accelerates delivery for technical users while establishing the robust platform needed for future capabilities.
Starting with declarative workflows enables technical users to begin testing automation patterns immediately while we gather feedback to inform future development. This foundation is essential for realizing our vision of completing the value chain from data to insight to automated outcomes.
Outcomes Enabled for Users
With this MVP, early adopters will be able to:
1. Validate Automation Patterns: Test workflow definitions using YAML to explore and validate automation approaches across security, observability, and business operations
2. Prove Core Triggers: Validate essential triggers including rules/alerts (Kibana rules, Security detection rules), schedules, and manual execution
3. Test Integration Ecosystem: Experiment with connectors through initial integrations to key external systems
4. Demonstrate End-to-End Value: Prove complete execution from trigger through actions with real business outcomes
5. Inform Future Development: Provide feedback on workflow structure, capabilities, and user experience to guide subsequent milestones
Example Validation Scenario
A user can create a basic workflow that:
* Triggers on a rule (e.g., Security detection rule)
* Queries Elasticsearch for context/enrichment 
* Sends a notification to Slack
* Executes successfully end-to-end


Note: This MVP focuses on proving technical feasibility and gathering learnings, not production-ready automation.
Key Capabilities & Scope
In Scope (Foundational)
* Workflow Engine: Core execution environment with support for Elastic Cloud Serverless
* Workflow Definition: YAML-based workflow definitions
* Basic UI: Workflow management (create, view, edit, delete), execution history
* Essential Triggers: Rule-based (Kibana rules, Security detection rules), scheduled, manual
* Connector Setup: Initial support for key providers
* Action Steps: Elastic actions (search, document operations), external system actions
* Flow Control Steps: Conditionals, loops, error handling
* Data Transformation Steps: Field access, basic transformations
Out of Scope (Future Milestones)
* Visual workflow builder
* Advanced monitoring and debugging
* Performance optimization
* AI-powered steps
* Workflow versioning/Git integration
Success Metrics
* Technical Validation: Core engine executes workflows reliably
* Integration Proof: Successful connection to Elastic and external systems
* Learning Goals: Gather feedback from 5-10 technical users on workflow patterns
* Foundation Ready: Platform stable enough to build upon for next milestone
Timeline
Target Delivery: TBD based on engineering assessment


* Technical design complete: By Friday 4th of July
* Development start: scheduled kickoff session for Monday, July 7th
* MVP release target ~date: TBD
Open Questions
1. Provider Integration Strategy: How do we integrate Keep's providers into Elastic? Which providers don't make sense to bring over at this stage  (e.g., Python, Bash, etc.)?
   1. Follow-up: Which subset should we prioritize for MVP?
   2. How will users configure connectors within Elastic?
2. Internal Elastic Actions: Which Elastic-native actions beyond basic search should we support initially?
   1. Can we create an extensible framework for other Elastic teams to contribute actions?
3. Schema Evolution: Should we modify Keep's YAML schema or adopt as-is initially?
4. Performance Targets: Specific latency/throughput requirements?
5. RBAC Scope: Should role-based access control be part of the initial MVP scope?




Appendix


FAQ: Tines Partnership & Keep Acquisition
Workflow Automation FAQ: Tines Partnership & Keep Acquisition
For Elastic Field Teams - Internal Use
Last Updated: Jun 30, 2025
Next Update: As roadmap details become available


Executive Summary
Strategic Positioning
Technical & Product Questions
Customer Scenarios
Messaging Guidelines


For questions not covered in this FAQ or complex customer scenarios, escalate to:
* Product Strategy: Tinsae Erkailo & James Spiteri
* Partnership Team: Justin van Wart
Executive Summary
Elastic is building native workflow automation capabilities (One Workflow) as part of our Search AI Platform while maintaining our existing Tines partnership. Customers can confidently invest in Tines for their automation needs today, and we'll continue supporting integrations with external automation platforms, like Tines, as part of our "open by design" philosophy.
Strategic Positioning
This section covers Elastic's overall strategy for workflow automation, how the Keep acquisition fits with our Tines partnership, and how to position our approach against competitive offerings.
Q: What is Elastic's strategy for workflow automation?
A: We're building native workflow automation capabilities that will enable outcomes like SOAR, AIOps, and business process automation directly within the Elastic platform. This completes the value chain from data to insight to automated action/outcome. However, we remain committed to our "open by design" principle. Customers can use our native capabilities, Tines, or other automation platforms based on their needs.


  

One Workflow Vision


Q: How does the Keep acquisition impact our Tines partnership?
A: The partnership continues unchanged. The acquisition of Keep brings us technology and talent to accelerate our native automation roadmap, while Tines remains a valuable integration/partnership for meeting customer automation needs today. 
Q: Are you building a SOAR platform to compete with Tines?
A: We're building workflow automation capabilities that enable SOAR outcomes, along with AIOps and other use cases across Search, Observability and Security. This isn't about competing with any specific vendor. It's about providing customers the option for deeply integrated automation within their data platform. Customers who prefer Tines, or any other third party automation platform, can continue using it.
Q: How should we position this against Splunk SOAR (Phantom), Datadog Workflows, or other integrated platforms?
A: Elastic will now natively include workflow automation capabilities as part of our offering, so customers no longer need external tools to bridge the data-to-action gap. We're providing complete automation capabilities out of the box that enable SOAR outcomes in Security, AIOps in Observability, and general business process automation across our solutions. This eliminates the integration complexity and vendor coordination that comes with stitching together multiple platforms.
Q: Is Elastic abandoning partnerships in favor of building everything in-house?
A: No. Partnerships remain core to our strategy. The Keep acquisition accelerates our efforts to introduce OOTB workflow automation capabilities. 
Q: What should we tell customers who ask about "vendor lock-in"?
A: Elastic's "open by design" philosophy means customers are never locked into using only our capabilities. While we will provide OOTB and open source workflow automation capabilities, customers are free to use alternatives. 
Technical & Product Questions
This section addresses specific questions about capabilities, timelines, and integration details for our native workflow automation solution.
Q: When will native workflow automation be available?
A: We're actively building foundational capabilities now. We'll share specific timelines as development progresses and milestones become clear. In the meantime, all existing integrations continue working as they do today.
Q: Will the native solution have a visual workflow builder (like Tines, n8n, etc,)?
A: Yes. Our roadmap includes visual workflow building along with other capabilities customers expect from modern automation platforms. We're designing for both technical users who prefer code-based approaches and analysts who prefer visual interfaces.
Q: Will there be pre-built automation content?
A: Yes. Our native solution will include out-of-the-box templates and workflows to help teams get started quickly, similar to what customers enjoy with other automation platforms today.
Q: How will I be able to migrate from my current automation platform to Elastic when your native capabilities are ready?
A: We'll provide migration tooling to ease the transition to our native automation capabilities, similar to our approach with SIEM migration. We understand that customers invest significant time building workflows, and we're committed to making any future transition as smooth as possible.
Q: How will this integrate with our existing Elastic deployment?
A: Native workflow automation will be built directly into the Elastic platform, available across our Search, Observability, and Security solutions. This enables automated outcomes without external tools or complex integrations.


  

Use Cases Enabled  


Q: Will this be available across all Elastic deployment models?
A: We aim to support Elastic Cloud Serverless, Elastic Cloud Hosted (ECH/ESS), and on-premises deployments. Availability for each deployment model will depend on development timelines, but our goal is to make native automation available across our various hosting options.
Customer Scenarios
This section provides specific guidance for common customer situations you'll encounter, including existing Elastic customers evaluating automation options and current Keep customers.
Q: We're an existing Elastic Security customer evaluating Tines for SOAR. Should we proceed?
A: Yes. Tines is an excellent solution for SOAR automation today. Your investment in building Tines workflows will be protected. The integrations will continue working, and the operational expertise you build transfers to any automation platform. Whether you start with Tines or wait for our native capabilities depends on your timeline and preferences.
Q: We're a Keep user. What happens to our investment?
A: You can continue using Keep as it is today. The integration of Keep into Elastic will introduce native workflow automation as part of the overall Elastic offering, whether you're using Elastic Security, Observability, or Search. This native automation will provide a much more comprehensive offering that enables you to go from data to insight to automated action/outcome within a single platform. You can continue to use Keep standalone, but our investment will be toward this more integrated offering that delivers deeper capabilities and a unified experience.
Q: We use another automation platform. Will you still integrate with us?
A: Our "open by design" philosophy means we support integrations with automation platforms where there's customer demand and technical feasibility. We're not building to lock customers into any specific approach. The goal is to provide flexibility while offering native capabilities for those who want deeper integration.
Messaging Guidelines
This section provides clear dos and don'ts for consistent messaging across all customer conversations about workflow automation.
Do Say:
* "We're building native workflow automation to complete the data-to-action/outcome value chain"
* "The Tines partnership continues"
* "You can start with external automation today or wait for our native capabilities"
* "Open by design means you're never locked into a single approach"
Don't Say:
* Specific timelines for native capabilities (we'll share when ready)
* That Tines or other platforms are temporary/deprecated
* That customers should wait for our native solution before starting automation
* That external integrations will be discontinued






FAQ: re:Invent
[WIP]
Workflow Automation FAQ: re:Invent
For Elastic Field Teams - Internal Use
Last Updated: Nov 27, 2025


Workflows Narrative
Executive Summary
Strategic Positioning
Technical & Product Questions
Customer Scenarios
Messaging Guidelines


For questions not covered in this FAQ or complex customer scenarios, escalate to:
* Product Strategy: Tinsae Erkailo 
* Engineering: Shahar Glazner & Tal Borenstein
Executive Summary
Elastic is building native workflow automation capabilities (One Workflow) as part of our Search AI Platform while maintaining our existing Tines partnership. Customers can confidently invest in Tines for their automation needs today, and we'll continue supporting integrations with external automation platforms, like Tines, as part of our "open by design" philosophy.
1. Overview & Positioning
Q: What is Workflow?
Workflows is Elastic’s native workflow automation platform, built directly into the Elasticsearch ecosystem, in Kibana. 
It enables organizations to automate actions and orchestrate multi-step processes using the data, signals, and insights already living in Elastic - no external vendor (SOAR/AIOps tool/etc.) required.
It completes the chain from data → insight → automated action/outcome.
  

One Workflow Vision


Q: Why is Elastic investing in native workflow automation now?
Because customers increasingly expect automation to be built into the platform where their data already lives. 
Competitors like Datadog, Dynatrace, Splunk, Google SecOps, and New Relic now offer native workflow engines. 
Elastic historically excelled at finding the signal, but customers had to use external tools to take action: creating a two-vendor problem.
Workflows removes that gap.
Q: Is Workflows a SOAR or AIOps product?
Not exactly. It is a foundational platform capability, not a separate product. 
It enables SOAR, AIOps, and business-process automation, but is not limited to any one of them. It supports automation across Security, Observability, and Search.  
Q: Does Workflows replace the Tines partnership or other automation vendors?
No. Elastic remains open by design.
Customers can keep using Tines or other workflow platforms. One Workflow simply gives them a deeply integrated option when native automation is preferred.
2. What Workflows Enables (Capabilities)
Q: What can customers automate with Workflows today?
Workflows supports:
* Triggers: rules, schedules, manual runs
* Action steps: Elastic-native actions (search, ES|QL, cases, Kibana APIs), external API calls
* Flow logic: branching, conditionals, loops.
* Transformations: data extraction, reshaping, filtering
* AI: summarization, classification, enrichment, and fully agentic reasoning steps (With Agent Builder)
* Composability: workflows triggering sub-workflows

Q: What makes Workflows unique compared to other workflow engines?
Three major differentiators:
1. Data-native execution
Automation happens where customer data already resides: eliminating data movement, ETL, and fragile integrations.
2. Elastic’s AI + search foundation
Workflows can call search, ES|QL, vector search, LLM reasoning, or AI agents with access to Elastic data and tooling.
3. Deep integration across Elastic solutions
Detections firing in Security → trigger workflows
APM anomalies in Observability → trigger workflows
Search analytics events → trigger workflows
No context-switching; no external services required.
Q: What does “agentic workflow” mean in Elastic?
Agentic workflows embed an AI agent as a step inside automation / workflows in agents.
The agent can:
   * gather context by querying Elastic
   * reason about ambiguous or incomplete information
   * generate hypotheses
   * choose next steps or tools
   * output structured guidance or actions
Workflows then execute the deterministic parts safely.
It combines AI judgment with workflow reliability.
3. Technical & Product Questions
This section addresses specific questions about capabilities, timelines, and integration details for our native workflow automation solution.
Q: When will native workflow automation be available?
A: We're actively building foundational capabilities now. We'll share specific timelines as development progresses and milestones become clear. In the meantime, all existing integrations continue working as they do today.
Q: Will the native solution have a visual workflow builder (like Tines, n8n, etc,)?
A: Yes. Our roadmap includes visual workflow building along with other capabilities customers expect from modern automation platforms. We're designing for both technical users who prefer code-based approaches and analysts who prefer visual interfaces.
Q: Will there be pre-built automation content?
A: Yes. Our native solution will include out-of-the-box templates and workflows to help teams get started quickly, similar to what customers enjoy with other automation platforms today.
Q: How will I be able to migrate from my current automation platform to Elastic when your native capabilities are ready?
A: We'll provide migration tooling to ease the transition to our native automation capabilities, similar to our approach with SIEM migration. We understand that customers invest significant time building workflows, and we're committed to making any future transition as smooth as possible.
Q: How will this integrate with our existing Elastic deployment?
A: Native workflow automation will be built directly into the Elastic platform, available across our Search, Observability, and Security solutions. This enables automated outcomes without external tools or complex integrations.


  

Use Cases Enabled  


Q: Will this be available across all Elastic deployment models?
A: Yes, from day one.
4. Use Cases Customers Care About
Q: What Security (SOAR-like) use cases does Workflows support?
   * Automated alert triage
   * External enrichment (VirusTotal, TI providers)
   * Automated containment (e.g., disable user, isolate host)
   * IOC sweeping
   * Secret leak response
   * Cloud posture remediation

Q: What Search / business automation scenarios are enabled?
      * Workflow-triggered document classification & routing
      * Customer support request triage
      * Content ingestion pipelines
      * Business logic automation for custom search apps

Q: We use another automation platform. Will you still integrate with us?
A: Our "open by design" philosophy means we support integrations with automation platforms where there's customer demand and technical feasibility. We're not building to lock customers into any specific approach. The goal is to provide flexibility while offering native capabilities for those who want deeper integration.
Messaging Guidelines
This section provides clear dos and don'ts for consistent messaging across all customer conversations about workflow automation.
Do Say:
         * "We're building native workflow automation to complete the data-to-action/outcome value chain"
         * "The Tines partnership continues"
         * "You can start with external automation today or wait for our native capabilities"
         * "Open by design means you're never locked into a single approach"
Don't Say:
         * Specific timelines for native capabilities (we'll share when ready)
         * That Tines or other platforms are temporary/deprecated
         * That customers should wait for our native solution before starting automation
         * That external integrations will be discontinued






PR
Keep in mind that this is meant to be an example and not the official press release. 
Elastic Announces “Elastic Workflows” to Power AI-Driven Automation Across its Search AI Platform
New native workflow automation enables customers to build intelligent workflows for advanced security operations, IT automation, and search-powered applications
SAN FRANCISCO-- Elastic (NYSE: ESTC), the Search AI Company, today announced Elastic Automate, native workflow automation capabilities integrated directly into the Elastic Search AI Platform. Elastic Automate empowers customers to orchestrate end-to-end processes, combining deterministic logic with AI-powered reasoning to accelerate outcomes across security, observability, and search.


Building on technology from Elastic's acquisition of Keep, Elastic Automate enables organizations to automatically respond to insights discovered within their Elastic deployments. Security teams can now build complete incident response workflows, IT teams can create self-healing infrastructure, and developers can automate business processes for their Search applications, all within a unified platform.


"Organizations generate tremendous insights from their data in Elastic, and now they can automatically act on those insights without switching contexts or managing integrations," said Ash Kulkarni, Chief Executive Officer at Elastic. "Elastic Automate represents a fundamental shift in how teams operationalize their data. By combining deterministic automation with AI reasoning in a single workflow, we're enabling a new generation of intelligent applications and automated outcomes"
Key Capabilities
Elastic Automate introduces powerful innovations for workflow automation:
         * Unified Automation Experience: Build and run workflows directly where data lives, with the same reliability and scale customers expect from Elastic
         * AI-Powered Workflow Steps: Embed AI agents within workflows to handle complex investigations and decisions that traditionally require human judgment
         * Natural Language Workflow Creation: Describe automation goals in plain English and let AI assist in building the implementation
         * Production Scale: Process millions of workflow executions with built-in monitoring, auditing, and governance
Transforming Operations Across Use Cases
Security Operations: Automate the complete threat lifecycle from detection to remediation. Early access customer Texas A&M University reduced mean time to response from 4 hours to 12 minutes by building AI-enhanced investigation workflows.


IT Operations: Transform reactive monitoring into proactive problem resolution. Create workflows that diagnose issues, identify root causes, and apply fixes before users are impacted.


Search Applications: Convert search results into immediate actions. Build applications where finding information automatically triggers multi-step business processes.


"Security teams need more than just alerts; they need intelligent automation that can investigate and respond at machine speed," said Santosh Krishnan, General Manager of Security and Observability at Elastic. "Elastic Automate enables teams to encode their expertise into workflows that combine the reliability of rules with the adaptability of AI, dramatically reducing the burden on analysts."
Built for the AI Era
Elastic Automate is designed from the ground up to harness the power of AI within automation workflows. Users can configure AI agents with specific personas, tool access, and decision boundaries, enabling workflows to handle novel situations while maintaining control and transparency.


"The ability to embed AI reasoning directly into our security workflows is transformative," said Michael Chen, CISO at Texas A&M University. "We're not just automating the routine anymore, we're scaling our team's investigative capabilities across every alert."
Availability
Elastic Automate is available today in Tech Preview for Elastic Cloud customers. General availability is planned for Q4 2025.


To learn more about Elastic Automate, read the blog post at _ or register for a demo at _.
About Elastic
Elastic (NYSE: ESTC), the Search AI Company, enables everyone to find the answers they need in real-time using all their data, at scale. Elastic's solutions for search, observability, and security are built on the Elastic Search AI Platform, the development platform used by thousands of companies, including more than 50% of the Fortune 500. Learn more at elastic.co.


Elastic and associated marks are trademarks or registered trademarks of Elastic N.V. and its subsidiaries. All other company and product names may be trademarks of their respective owners.
Media Contact
Elastic PR
PR-team@elastic.co


Source: Elastic N.V.




YAML Spec
RFC: One Workflow YAML Spec


Date: July 2025
YAML Version: 1
TTL: August 4rd
Status: Resolved
________________




Reviewer
	Comments
	Approved?
	…
	…
	LGTM
	Mike Cote
	Left a comment about “triggers” but overall LGTM.
	LGTM
	Tia Milosevic
	

	LGTM
	

________________


Executive Summary
This RFC proposes a declarative YAML specification for defining and executing automation workflows within the Elastic platform. The spec powers the Keep integration into Elastic, enabling teams to build complex, branching, event-driven logic: such as alert triage and incident response - through a structured YAML format.
This document outlines the YAML spec, its behavior at runtime, and how it integrates with the One Workflow framework in Kibana.
Elastic lacked a unified, developer and user-friendly format for defining automation workflows across its solutions. 
Current approaches were either tool-specific or required imperative code, limiting reusability and inhibiting low-code adoption.
The Keep acquisition addressed this gap by introducing native workflow automation capabilities to Elastic. 
While SOAR in the Security solution is the initial use case, the long-term vision spans the entire Elastic platform, including Security, Observability (with extensibility into future AIOps scenarios, for example), and Search. The One Workflow YAML spec is a central pillar of that strategy.
________________


Problem Statement
Operations teams across Security, Observability, and IT are overwhelmed by the volume of data, alerts, and manual tasks required to maintain system health and security. This leads to:
         * Analyst Burnout: Repetitive, manual investigation and triage tasks lead to fatigue and errors.
         * Slow Response Times: The time required to coordinate across teams and tools for incident response or root cause analysis is too long.
         * Missed Insights: Valuable signals are lost because teams lack the capacity to investigate everything.
What’s missing is a native, unified automation layer that bridges Elastic's observability, security, and AI-driven insights with actionable, in-product automated outcomes. Keep's acquisition enables Elastic to fill this gap.
The One Workflow YAML Spec is a foundational piece: a machine-readable (LLM-friendly), developer-friendly format for authoring workflows that:
         * Are portable and versionable (via YAML)
         * Support branching logic, loops, interpolation, and step re-use
         * Integrate natively with Kibana's connectors
         * Power both code-first and UI-driven automation experiences
________________


Goals
         * Define a schema-backed YAML spec to describe workflows declaratively
         * Support triggers, conditional logic, loops, and branching
         * Integrate with Kibana connector execution
         * Enable observability via structured execution events
________________


Proposal
Spec Overview
A workflow is defined in YAML with the following top-level fields:
         * name, description, enabled, version
         * triggers: e.g. manual, scheduled
         * steps: Executable nodes that define the logic
         * settings: Optional metadata for execution behavior (see below)
Global Settings
The optional settings section allows configuring runtime-wide options:
settings:
 retry:
    attemps: 3
    delay: 10
    strategy: 'fixed' # could be exponential_backoff in the future
 timezone: 'UTC'  # or 'America/New_York', etc.
         * retry: Applies as defaults for steps that don’t override their own retry policy.
         * timezone: Standard IANA timezone to localize time-based expressions. Defaults to ‘UTC’


Example
version: 1
name: "Phishing Alert Investigation"
description: "Workflow to detect if an alert is a real phishing attempt or not"
enabled: true
triggers:
- type: manual
inputs:
- name: url
  description: The url of the scanning service
  type: string
  default: https://www.phishingemailsaddress.com
steps:
    - name: Parallel analysis
      type: parallel
      branches:
        - name: URL scan
          steps:
            - name: URL scan
              type: http.get
              with:
                url: "{{inputs.url}}?query=randomemail@addresss.com"
        - name: Classification
          steps:
            - name: Classification
              connectorId: "inference-open-ai-uuid"
              type: openai.completion
              with:
                prompt: "Classify the email content for phishing risk."
    - name: Merge results
      type: merge
      sources:
        - "Parallel analysis.URL scan"
        - "Parallel analysis.Classification"
      steps:
        - name: AI summary
          connectorId: "inference-open-ai-uuid"
          type: openai.completion
          with:
            prompt: "Determine if the email is malicious or not"
            structuredOutput:
              malicious: bool # true/false
        - name: If malicious
          type: if
          condition: "{{steps.AI summary.malicious}}"
          steps: # true
            - name: Create case
              type: kibana.createCase
              with:
                description: "Create a new case for the phishing alert"
            - name: Add to case
              type: kibana.addToCase
              with:
                description: "Add alert details to the case"
            - name: Send message
              type: slack
              connector-id: "slack-uuid"
              with:
                channel: "#alerts_to_investigate"
                message: "Potential phishing alert detected and case created. Please investigate."
          else:
            - name: Add note
              type: kibana.addNote
              with:
                description: "Alert determined not to be phishing. Added note to alert."
            - name: Close alert
              type: kibana.closeAlert
              with:
                description: "Alert closed as not malicious."
    - name: Workflow completion notification
      type: slack
      connector-id: "slack-uuid"
      if: always()
      with:
        channel: "#workflow-notifications"
        message: "Phishing alert investigation completed. Result: {{'MALICIOUS - Case created' if steps.AI summary.structuredOutput.malicious else 'NOT MALICIOUS - Alert closed'}}"

  



________________

Terminology
         * Trigger: Defines when a workflow runs.
         * Step: A logical unit of work (node).
         * Workflow: A set of named, executable steps.
         * Interpolation: Dynamic variable substitution using {{context.path}}, powered by our templating engine.
         * Branch: A fork in execution (parallel or conditional).
________________


Detailed Design
Step Ordering and Execution Model
The One Workflow YAML specification defines workflows as sequential, meaning the order of steps listed in the YAML is critical. Each step executes in the order it appears, unless a control structure (e.g., if, parallel, foreach) dictates otherwise. This is similar to how GitHub Actions workflows behave.
For example, referencing a variable or context entry from a future step (i.e., one that hasn't run yet) will not resolve correctly. Workflow authors must ensure that the structure respects data dependencies and logical execution flow.
Step Types (Primitives)
Each step in a workflow can be one of the following:
         * Control flow primitives (see here for MVP scope)
         * if: Conditional branching
         * KQL expression returning true\false
         * https://github.com/elastic/security-team/issues/13176
         * foreach: Iterate over lists
         * https://github.com/elastic/security-team/issues/13177
         * parallel: Execute branches concurrently
         * https://github.com/elastic/security-team/issues/13182
         * merge: Synchronize multiple branches
         * https://github.com/elastic/security-team/issues/13178
         * loop: Repeat a step or sequence
         * https://github.com/elastic/security-team/issues/13183
         * Single connector invocation:
These are atomic actions, often mapped directly to Kibana or third-party connectors (via connectorId, for example). 
            * https://github.com/elastic/security-team/issues/13184
            * Examples:
            * slack
            * slack.react
            * kibana.createCase
            * openai.completion
            * http.request
            * elastic.closeAlert
A single YAML-defined step must either be a structural construct (if, foreach, etc.) or a terminal node invoking one of these connectors.
Conditional Execution & Built-in Functions
To support expressive control over when steps run (similar to how GitHub Actions does it), the workflow engine supports built-in conditional helpers that can be used within if expressions. These allow for more readable and reusable conditions across workflows.
IF conditions are expressed in KQL, which was selected for the following reasons:
            * Clear boolean logic
            * KQL parser already exists (just evaluator is needed)
            * KQL spec already exists
            * KQL is known by Kibana users
            * No need for users to learn one more syntax for filtering
Function
	Description
	always()
	Returns true unconditionally; useful for catch-all or fallback branches
	success()
	Evaluates to true if the previous step(s) succeeded
	failure()
	Evaluates to true if the previous step(s) failed
	canceled()
	Evaluates to true if a previous step was explicitly cancelled
	Example
- name: Notify fallback
  type: slack
  connectorId: 'uuid-slack-send'
  if: 'always()'
  with:
    message: 'Workflow reached final step, regardless of outcome.'
    channel: '{{consts.INFOSEC_CHANNEL}}'
These functions can be combined with other logic in templating expressions. For example:
if: '{{failure() && steps.previous_step.error_code == 503}}'

Step Identifiers and Naming
Each step in a workflow must have a unique name. This functions as the step's identifier (similar to how name is handled in Keep today) and is used to reference outputs and state from other steps.
This uniqueness enables reliable referencing throughout the YAML via interpolation expressions like:
{{steps.get_user_info.results.email}}
This name uniqueness is enforced during schema validation and runtime execution.


Schema
The schema supports primitive and structured input/output types:
            * string, number, boolean, object, array, conversation, alert
Interpolation & Templating Engine
The interpolation engine supports:
            * Full value substitution: {{foo}}
            * String embedding: "Hello {{user.name}}"
            * Complex templating logic, powered by a hybrid engine combining Mustache-style syntax and Nunjucks (a TypeScript-compatible port of Python's Jinja2)
This allows developers to use rich logic inside strings, including filters, loops, and conditionals. For example:
{% if event.severity == 'high' %}
  🚨 High severity event: {{ event.name }}
{% else %}
  ✅ Event handled: {{ event.name }}
{% endif %}
You can also apply filters or expressions inline:
{{ event.timestamp | date("YYYY-MM-DD") }}

The templating engine ensures safe resolution, supports default fallbacks, and stringifies deeply nested values on demand. All templated expressions are evaluated in the context of the current step’s execution environment, which includes:
            * inputs
            * trigger payload
            * outputs from previous steps
            * Execution metadata 
            * Workflow metadata
This mechanism underpins the full power of the YAML spec and allows declarative workflows to remain flexible and expressive.
Interpolation context includes inputs, prior steps, trigger payloads, and metadata. The templating engine supports safe resolution, default fallbacks, and stringification of nested values.


import nunjucks from 'nunjucks';
import Mustache from 'mustache';


export class TemplatingEngine {
  public render(
    syntax: 'mustache' | 'nunjucks',
    template: string,
    context: Record<string, any>
  ): string {
    switch (syntax) {
      case 'nunjucks':
        return this.renderNunjucks(template, context);
      case 'mustache':
        return this.renderMustache(template, context);
      default:
        throw new Error(`Unsupported syntax: ${syntax}`);
    }
  }


  private renderNunjucks(template: string, context: Record<string, any>): string {
    return nunjucks.renderString(template, context);
  }


  private renderMustache(template: string, context: Record<string, any>): string {
    // Assuming Mustache is available globally or imported
    return Mustache.render(template, context);
  }
}

Nesting
            * if and foreach steps support steps and else branches.
            * parallel supports isolated branches with independent steps.
________________


Telemetry
            * Track: workflow executions, durations, step results, errors.
            * Usage insights to inform schema evolution and feature prioritization.
________________


Testing
            * Schema validation tests
            * Parser and interpolation unit tests
            * End-to-end integration with workflow execution API
            * Error scenarios and invalid YAML cases
            * Performance & Scaling Guarantees


________________


Examples
If on a step level


- name: Report success
  type: slack
  connectorId: 'uuid-slack-send-5678'
 if: '{{steps.send_reminder.results | len}} > 0'
  with:
     message: 'Successfully sent reminder to {{steps.send_reminder.results | len}} users'
     channel: '{{consts.INFOSEC_CHANNEL}}'

Nested If


- name: Check if alert is malicious
  type: if
  condition: "{{alert.is_malicious}}"
  steps:
     - name: Create case
       type: create_case
     - name: Add to case
       type: add_to_case
     - name: Notify team
       foreach: "{{alert.teams}}"
         steps:
           - name: Send message
             type: slack
             with:
               message: "Alert for {{foreach.value.teamName}}"
               channel: "{{foreach.value.channel}}"



Handling failure


- name: AI Summary
  type: openai.completion
  connectorId: '6f8af2b0-7502-4966-a0db-644d24bf052e' 
  with:
    prompt: "Determine if the following email is malicious or not: {{alert.message}}"
    structuredOutput:
      malicious: bool
      reason: string
      confidence: number
  on-failure:
    fallback-step:
      name: AI Summary (Anthropic)
      type: anthropic.completion
      connectorId: '615add2a-5f89-4a7e-a7e0-9e62f42ad200' 
      with:
        prompt: "Determine if the following email is malicious or not: {{alert.message}}"
        structuredOutput:
          malicious: bool
          reason: string
          confidence: number

Open Items
Workflow Outputs (TBD)
Currently, workflows in Keep are designed to execute asynchronously, and do not expose a formal outputs declaration. However, in future, synchronous execution is planned, enabling workflows to return structured outputs upon completion.
For sync workflows, the engine would support a timeout-based blocking call and expose the final merged result of all steps. [ag][ah]
Example output:
{
  "Parallel analysis": {
    "URL scan": { ... }
  },
  "Merge results": {
    ...
  }
}
This could allow calling workflows from other workflows, or external systems, with a clearly defined contract.
Design details (e.g., timeout, flattening, typing) are TBD, pending discussion and alignment with product.
1chat Invocation (TBD)
Onechat will have a number of tools and agents that can be executed. Some of these tools will be out of the box and also user created with ESQL. 


Onechat will also have a default agent that is assigned to all tools plus ability for users to create custom agents that are linked to specific tools and have custom instructions.  
Example Tool Execution:
- name: search hr
  type: chat.tools.X        
  with:
    query: '{{query}}'
    index: 'hr_documents'
Example Agent Execution:
- name: ask_question
  type: chat.agents.default
  with:
    input: '{{question}}'
    agent_id: 'my_agent'
Challenges
Scoping the user to the agent / tool execution
Tools and agents are scoped to individual user requests, inheriting the user's data access permissions. I'm assuming TaskManager accounts for this permission binding? 




Documentation (Internal Preview)
Workflows Internal (Elastic) Preview Documentation
⚠️ INTERNAL PREVIEW - CONFIDENTIAL
This documentation is for internal Elastic team members only. DO NOT share with external customers. Public-facing documentation will be worked on in collaboration with the docs team closer to milestone 2 (December 2025)
Welcome to Elastic Workflows
This documentation will help you get started with Elastic Workflows.


Workflows are Elastic's native workflow automation functionality, built to close the gap between insight and outcome. Workflows are the "hands" for our platform's "senses" (Data Foundation[ai]) and "brain" (Search AI Platform). Learn more about the vision for workflows here. 
About This Preview
This is an early, internal-only release with two critical goals:


            * Validate the engine's reliability. We need to prove the core engine can execute workflows consistently and at scale without failure.
            * Validate the core primitives. We need to confirm that our first set of building blocks (the initial triggers, steps, and connectors) are the right ones to solve foundational automation use cases across Security, Observability, and Search.


To achieve this, we are starting with a declarative, YAML-based authoring experience built for technical practitioners. The visual, drag-and-drop canvas will come later, once this foundation is proven.


Your feedback is not just helpful; it's critical. You are our first and most important users ❤️


Use Workflows to automate the painful, repetitive parts of your job. Tell us what's missing. Show us where it breaks. Your input will directly shape the future of Workflow Automation at Elastic.
📞 Need Help
            * Primary Contacts: 
            * Product: Tinsae Erkailo (Slack: @tin) 
            * Eng Lead: Tal Borenstein (Slack: @tal) 
            * Tech Lead: Shahar Glazner (slack: @shahar.glazner)
            * Slack Channel: #one-workflow
            * Questions? Don't hesitate to reach out in our Slack channel!
📚 Documentation Section
            * Getting Started 
            * Enable Workflows 🚀
            * Creating your first workflows[aj] 🌱
            * Triggers
            * Flow Controls
            * Actions
            * Data & Error Handling 






Enable workflows
Getting Started with Workflows
Prerequisites
Before you begin, make sure you have:
            * Access to an Elastic Stack deployment (version 9.3-snapshot or serverless (QA or Production) env)
            * Admin privileges in Kibana
            * Access to Kibana DevTools




Elastic Cloud Hosted (ECH)
To use workflows in ECH you need first to deploy a snapshot based version (which can be found in GCP, Los Angeles region) when you login with your @elastic.co account.


Remember, this is an internal preview. If you run into any issues:
            * Slack Channel: #one-workflow
            * Quick questions: Drop them in the Slack channel anytime!


Ready? Let's start by enabling workflows… 
Enable Workflows
Note: If you are trying to enable workflows before 9.3 snapshots are available, i.e., prior to this internal preview, please follow the instructions linked here: Pre internal preview enablement instructions
Step 1: Access Dev Tools
            1. Log into your Kibana instance
            2. Navigate to Dev Tools from the main menu
            1. In classic navigation: Look for "Dev Tools" in the left sidebar
            2. In solution navigation: Find it under the hamburger menu
Step 2: Enable the Feature Flag
The Workflows feature is disabled by default. To enable it, run the following command in Dev Tools:






POST kbn://internal/kibana/settings
{
  "changes": {
    "workflows:ui:enabled": true
  }
}
	You should see a response like:


{
  "acknowledged": true
}
	

Step 3: Access the Workflows UI
Once enabled, you can access Workflows (Depending on your side nav, configuration, or server list, workflows will appear in the following places):


o11y:


  

Security:                
  

Search:
  



Classic (under Management Section):
  



Getting Help
If you encounter any issues or errors, reach out to us in the #one-workflow Slack channel.
Next Steps
Great! Workflows is now enabled. Let's create your first workflow….




Your First Workflow
Your First Workflow
Let's create and run your first workflow!
What We'll Build
We'll create a workflow that indexes and searches through National Parks data, demonstrating the core concepts and capabilities of Workflows along the way.
Step 1: Navigate to Workflows
            1. Open your Kibana instance
            2. Go to /app/workflows (or click "Workflows" in the navigation menu)
Step 2: Create a New Workflow
            1. Click the "Create Workflow" button
            2. You'll see the YAML editor interface:


[Screenshot placeholder - YAML editor interface]
Step 3: Define Your Workflow
Remove the placeholder content and copy and paste the YAML below into the editor:


name: 🏔️ National Parks Demo
description: Creates an Elasticsearch index, loads sample national park data using bulk operations, searches for parks by category, and displays the results.
enabled: true
tags: ["demo", "getting-started"]
consts:
 indexName: national-parks
triggers:
 - type: manual
steps:
 - name: get_index
   type: elasticsearch.indices.exists
   with:
     index: "{{ consts.indexName }}"
 - name: check_if_index_exists
   type: if
   condition: 'steps.get_index.output : true'
   steps:
     - name: index_already_exists
       type: console
       with:
         message: "index: {{ consts.indexName }} already exists. Will proceed to delete it and re-create"
     - name: delete_index
       type: elasticsearch.indices.delete
       with:
         index: "{{ consts.indexName }}"
   else:
     - name: no_index_found
       type: console
       with:
         message: "index: {{ consts.indexName }} Not found. Will proceed to create"
      
 - name: create_parks_index
   type: elasticsearch.indices.create
   with:
     index: "{{ consts.indexName }}"
     mappings:
       properties:
         name: { type: text }
         category: { type: keyword }
         description: { type: text }
 - name: bulk_index_park_data
   type: elasticsearch.bulk
   with:
     index: "{{ consts.indexName }}"
     operations:
       - name: "Yellowstone National Park"
         category: "geothermal"
         description: "America's first national park, established in 1872, famous for Old Faithful geyser and diverse wildlife including grizzly bears, wolves, and herds of bison and elk."


       - name: "Grand Canyon National Park"
         category: "canyon"
         description: "Home to the immense Grand Canyon, a mile deep gorge carved by the Colorado River, revealing millions of years of geological history in its colorful rock layers."


       - name: "Yosemite National Park"
         category: "mountain"
         description: "Known for its granite cliffs, waterfalls, clear streams, giant sequoia groves, and biological diversity. El Capitan and Half Dome are iconic rock formations."
        
       - name: "Zion National Park"
         category: "canyon"
         description: "Utah's first national park featuring cream, pink, and red sandstone cliffs soaring into a blue sky. Famous for the Narrows wade through the Virgin River."
        
       - name: "Rocky Mountain National Park"
         category: "mountain"
         description: "Features mountain environments, from wooded forests to mountain tundra, with over 150 riparian lakes and diverse wildlife at various elevations."
 - name: search_park_data
   type: elasticsearch.search
   with:
     index: "{{ consts.indexName }}"
     size: 5
     query:
       term:
         category: "canyon"
 - name: log_results
   type: console
   with:
     message: |-
       Found {{ steps.search_park_data.output.hits.total.value }} parks in category "canyon".
 - name: loop_over_results
   type: foreach
   foreach: "{{steps.search_park_data.output.hits.hits | json}}"
   steps:
     - name: process-item
       type: console
       with:
         message: "{{foreach.item._source.name}}"[ak]


Step 4: Save Your Workflow
            1. Click the "Save" button at the top right
            2. Your workflow is now created and ready to run


  

Step 5: Run Your Workflow
            1. Click the "Run" button (▶️ play icon) next to the Save button
            2. The workflow will start executing immediately


  

Step 6: Monitor Execution
As your workflow runs, you'll see execution logs appear in a side panel on the right:
            1. Real-time execution logs - Each step appears as it executes
            2. Step status indicators - Green checkmarks for success, timestamps for duration
            3. Expandable step details - Click any step to see input, output, and timeline


  

Step 7: View Execution History
To see all past executions:
            1. Click the "Executions" tab
            2. View a list of all workflow runs (including pending & and in progress executions) along with their status and completion time[al]
            3. Click any execution to see its detailed logs


  



Clicking into the failed execution, we see exactly why it failed (we already created an index with the same name doh!)
  



Understanding What Happened
Let's examine each part of our first workflow above to understand how it works:
Workflow Metadata
name: 🏔️ National Parks Demo
description: Creates an Elasticsearch index, loads sample national park data using bulk operations, searches for parks by category, and displays the results.
enabled: true
tags: ["demo", "getting-started"]

            * name: A unique identifier for your workflow
            * description: Explains the workflow's purpose (shown in the UI)
            * enabled: Controls whether the workflow can be executed (set to false to disable)
            * tags: Labels for organizing and finding workflows
Constants
consts:
  indexName: national-parks-data

            * consts: Defines reusable values that can be referenced throughout the workflow
            * Accessed using template syntax: {{ consts.indexName }}
            * Promotes consistency and makes workflows easier to maintain
Trigger
triggers:
  - type: manual

            * triggers: Defines how the workflow starts
            * manual: Requires user action (clicking Run button)
            * Other trigger types (scheduled, alert-based) will be covered in the Triggers section
Step 1: Create Index
- name: create_parks_index
  type: elasticsearch.indices.create
  with:
    index: "{{ consts.indexName }}"
    settings:
      number_of_shards: 1
      number_of_replicas: 0
    mappings:
      properties:
        name: { type: text }
        category: { type: keyword }
        description: { type: text }

            * Step Type: This is an internal action step that directly interacts with Elasticsearch
            * Purpose in Workflow: Establishes the data structure for our park information, ensuring fields are properly typed for searching and aggregation
            * Key Elements:
            * Uses elasticsearch.indices.create - a built-in action that maps to the Elasticsearch Create Index API
            * Defines mappings to control how data is indexed (text for full-text search, keyword for exact matching)
            * References the constant indexName for consistency
            * Sets index settings for optimal performance in this demo


Learn more about internal action steps in the Internal Actions section
Step 2: Bulk Index Documents
- name: bulk_index_park_data
  type: elasticsearch.bulk
  with:
    index: "{{ consts.indexName }}"
    operations:
      - name: "Yellowstone National Park"
        category: "geothermal"
        description: "America's first national park, established in 1872..."
      - name: "Grand Canyon National Park"
        category: "canyon"
        description: "Home to the immense Grand Canyon..."
      # ... additional parks

            * Step Type: Another internal action step using Elasticsearch's bulk API
            * Purpose in Workflow: Efficiently loads multiple documents in a single operation, populating our index with sample data
            * Key Elements:
            * The operations array contains the documents to index
            * Each document becomes a searchable record in Elasticsearch
            * Uses the field names defined in our mappings (name, category, description)
            * Each document becomes a searchable record with consistent field structure
            * This step demonstrates how to handle batch operations in workflows
Step 3: Search Parks
- name: search_park_data
  type: elasticsearch.search
  with:
    index: "{{ consts.indexName }}"
    size: 5
    query:
      term:
        category: "canyon"

            * Step Type: Internal action step for querying Elasticsearch
            * Purpose in Workflow: Retrieves specific data based on criteria, demonstrating how workflows can make decisions based on data
            * Key Elements:
            * Searches for parks with category "canyon" (will find Grand Canyon and Zion)
            * Results are automatically available to subsequent steps via steps.search_park_data.output
            * Limits results to 5 documents for manageable output
            * Shows how workflows can filter and process data dynamically
Step 4: Log Results
- name: log_results
  type: console
  with:
    message: |-
      Found {{ steps.search_park_data.output.hits.total.value }} parks in category "canyon".
      Top results: {{ steps.search_park_data.output.hits.hits | json(2) }}

            * Step Type: A console step for output and debugging
            * Purpose in Workflow: Presents the results in a human-readable format, demonstrating how to access and format data from previous steps
            * Key Elements:
            * Template variables access the search results: {{ steps.search_park_data.output }}
            * The | json(2) filter formats JSON output with indentation
            * Uses the exact step name search_park_data to reference previous step output
            * Shows how data flows through the workflow and can be transformed
Key Concepts Demonstrated
This workflow introduces several fundamental concepts:


            * Internal Action Steps: Built-in steps that interact with Elasticsearch and Kibana APIs
            * Data Flow: How information moves from step to step using outputs and template variables
            * Constants: Reusable values that make workflows maintainable
            * Template Syntax: The {{ }} notation for dynamic values
            * Step Chaining: How each step builds on previous ones to create a complete process
What's Next?
Now that you have a working workflow, you're ready to explore more advanced features. In the following sections, we'll build upon this National Parks example to demonstrate:


            * Triggers - Automate when this workflow runs (daily reports, alert responses)
            * Control Flow - Add conditional logic based on search results
            * External Actions - Send notifications about park data
            * AI Integration (to do) - Generate summaries or enrich park descriptions
            * Data & Error Handling - Make the workflow resilient to failures


Each section will extend this workflow with new capabilities, helping you learn by building on what you already understand.


Ready to learn more? Continue to Core Concepts[am] to understand how workflows operate, or jump to Triggers to automate when your workflows run.


Questions? Ask in #one-workflow




Core Concepts - Workflow Triggers
Triggers
Triggers determine when and how your workflows start executing. Every workflow must have at least one trigger defined.
Available Trigger Types
During tech preview, three trigger types are available (more to come in the future):
Manual Trigger
Use when: You want to run workflows on-demand through the UI or API


Manual triggers require explicit user action to start the workflow. This is perfect for:
            * Testing and development
            * One-off data processing tasks
            * Administrative actions
            * Workflows that need human decision to start
Scheduled Trigger
Use when: You want workflows to run automatically at specific times or intervals


Scheduled triggers run workflows based on time, supporting both:
            * Interval-based: Run every X minutes/hours/days
            * Cron expressions: Run at specific times (e.g., daily at 2 AM)


Perfect for:
            * Daily reports
            * Regular data cleanup
            * Periodic health checks
            * Scheduled data synchronization
Alert Trigger
Use when: You want workflows to respond to Elastic alerts


Alert triggers fire when a detection rule or alerting rule generates an alert. The workflow receives the alert context, including all alert fields and values.


Perfect for:
            * Alert enrichment and triage
            * Automated incident response
            * Case creation and assignment
            * Notification routing based on alert severity
Quick Example
Here's how each trigger type looks in your workflow:


# Manual trigger
triggers:
  - type: manual


# Scheduled trigger (every 5 minutes)
triggers:
  - type: scheduled
    with:
      interval: 5m


# Alert trigger
triggers:
  - type: alert
	

Trigger Context
Each trigger type provides different data to the workflow context through the event field:
            * Manual: User information and any parameters passed
            * Scheduled: Execution time and schedule information
            * Alert: Complete alert data including fields, severity, and rule information


Access trigger data in your workflow using template variables:


steps:
  - name: logTriggerInfo
    type: console
    with:
      message: "Workflow started at {{ execution.startedAt }}"
      details: "Event data: {{ event | json(2) }}"
	

Next Steps
Learn how to configure each trigger type:
            * Manual Triggers - On-demand execution
            * Scheduled Triggers - Time-based automation
            * Alert Triggers 




	

Questions? Ask in #one-workflow or contact Tin (@tin)




Scheduled Triggers
Scheduled Triggers
Run workflows automatically at specific times or intervals.
How Scheduled Triggers Work
Scheduled triggers execute workflows based on time, without requiring manual intervention. They support:
            * Interval-based scheduling: Run every X minutes/hours/days
            * RRules expressions: Run at specific times (e.g., daily at 2 AM)
            * Timezone awareness: Schedule based on specific timezones
Interval-Based Scheduling
Basic Syntax


triggers:
  - type: scheduled
    with:
      interval: "<amount:number><dimension:s|m|h|d>"
	

Supported Intervals
            * Seconds: 30s (minimum 30 seconds)
            * Minutes: 5m, 15m, 30m
            * Hours: 1h, 6h, 12h
            * Days: 1d, 7d
Examples


# Every 5 minutes
triggers:
  - type: scheduled
    with:
      interval: "5m"


# Every hour
triggers:
  - type: scheduled
    with:
      interval: "1h"


# Every day (24 hours)
triggers:
  - type: scheduled
    with:
      interval: "1d"


# Every week
triggers:
  - type: scheduled
    with:
      interval: "7d"
	

Cron-Based Scheduling
Basic Syntax


triggers:
  - type: scheduled
    with:
      rrule:
        freq: DAILY
        interval: 1
        tzid: UTC
        dtstart: 2024-01-15T09:00:00Z
        byhour: []
        byminute: []
        byweekday: []
        bymonthday: []


	



Required Fields
            * freq: Frequency type - ‘DAILY’, ‘WEEKLY’, or ‘MONTHLY’
            * interval: Positive integer for interval (e.g., 1 for every day, 2 for every 2 days)
            * tzid: Timezone identifier (e.g., ‘UTC’, ‘America/New_York’, ‘Europe/London’)
Optional Fields
            * dtstart: Start date in ISO format (e.g., ‘2024-01-15T09:00:00Z’)
            * byhour: Array of hours (0-23) when to run
            * byminute: Array of minutes (0-59) when to run
            * byweekday: Array of weekdays (‘MO’, ‘TU’, ‘WE’, ‘TH’, ‘FR’, ‘SA’, ‘SU’)
            * bymonthday: Array of month days (1-31, negative for end of month)
Validation Rules
            * WEEKLY frequency requires at least one byweekday value
            * MONTHLY frequency requires at least one bymonthday value
            * byhour values must be between 0 and 23
            * byminute values must be between 0 and 59
            * dtstart must be a valid ISO date string


RRule Examples
Daily Scheduling
Daily at 9 AM UTC


triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'UTC'
        byhour: [9]
        byminute: [0]

Daily at multiple times (6 AM, 12 PM, 6 PM)
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'UTC'
        byhour: [6, 12, 18]
        byminute: [0]

Daily at 2:30 PM EST
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'America/New_York'
        byhour: [14]
        byminute: [30]

Weekly Scheduling
Every Monday and Friday at 2 PM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'WEEKLY'
        interval: 1
        tzid: 'UTC'
        byweekday: ['MO', 'FR']
        byhour: [14]
        byminute: [0]

Every weekday (Monday-Friday) at 8 AM and 5 PM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'America/New_York'
        byweekday: ['MO', 'TU', 'WE', 'TH', 'FR']
        byhour: [8, 17]
        byminute: [0]

Every Tuesday at 10:30 AM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'WEEKLY'
        interval: 1
        tzid: 'UTC'
        byweekday: ['TU']
        byhour: [10]
        byminute: [30]

Monthly Scheduling
Monthly on 1st and 15th at 10:30 AM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'MONTHLY'
        interval: 1
        tzid: 'UTC'
        bymonthday: [1, 15]
        byhour: [10]
        byminute: [30]

Monthly on the last day of the month at 11 PM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'MONTHLY'
        interval: 1
        tzid: 'UTC'
        bymonthday: [-1]  # Last day of month
        byhour: [23]
        byminute: [0]

Custom Start Date
Daily workflow starting from a specific date
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'UTC'
        dtstart: '2024-01-15T09:00:00Z'
        byhour: [9]
        byminute: [0]

Complex Scheduling Patterns
Business hours monitoring (weekdays at 8 AM and 5 PM EST)
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'DAILY'
        interval: 1
        tzid: 'America/New_York'
        byweekday: ['MO', 'TU', 'WE', 'TH', 'FR']
        byhour: [8, 17]
        byminute: [0]

Every 2 weeks on Monday at 9 AM
triggers:
  - type: 'scheduled'
    with:
      rrule:
        freq: 'WEEKLY'
        interval: 2
        tzid: 'UTC'
        byweekday: ['MO']
        byhour: [9]
        byminute: [0]



Manual Triggers
Manual Triggers
Execute workflows on-demand through the UI or API.
How Manual Triggers Work
Manual triggers require explicit user action to start a workflow. They're perfect for:
            * Testing and development
            * One-off data processing
            * Administrative tasks
            * Workflows requiring human decision to start
Basic Configuration
The simplest trigger configuration:


triggers:
  - type: manual
	This allows the workflow to be executed by:
            * Clicking the "Run" button in the Workflows UI
            * Calling the workflow execution API
            * Triggering from other systems via API
Executing Manual Workflows
Via UI
            1. Navigate to your workflow in /app/workflows
            2. Click the Run (▶️) button
            3. The workflow executes immediately
            4. View real-time logs in the execution panel
Passing Input Parameters
Manual triggers can accept input parameters that are available throughout the workflow execution.
Defining Inputs


name: Manual Processing Workflow
inputs:
  - name: environment
    type: string
    required: true
    default: "staging"
    description: "Target environment for processing"
  
  - name: batchSize
    type: number
    required: false
    default: 100
    description: "Number of records to process"
  
  - name: dryRun
    type: boolean
    required: false
    default: true
    description: "Run in test mode without making changes"


triggers:
  - type: manual


steps:
  - name: validateInputs
    type: console
    with:
      message: |
        Starting workflow with:
        - Environment: {{ inputs.environment }}
        - Batch Size: {{ inputs.batchSize }}
        - Dry Run: {{ inputs.dryRun }}        
	



Alert Triggers
Alert Triggers
Alert triggers enable workflows to automatically respond when Elastic detection rules or alerting rules fire.
How Alert Triggers Work
Alert triggers create a bridge between Elastic's alerting framework and Workflows:
            1. Define the trigger in your workflow YAML
            2. Configure the alert rule to run your workflow as an action
            3. When the alert fires, your workflow automatically executes with the alert context
Step 1: Create a Workflow with Alert Trigger
First, create your workflow with an alert trigger:


name: Security Alert Response
description: Enriches and triages security alerts
enabled: true
triggers:
  - type: alert
steps:
  ....
	

Step 2: Configure the Alert Rule
After creating your workflow, you need to configure your alert rule to trigger it:
For Security Detection Rules
            1. Navigate to Security → Rules
            2. Find or create the detection rule you want to trigger the workflow
            3. Edit the rule and go to the Actions section
            4. Click Add action
            5. Select Run Workflow from the action types


[Screenshot placeholder - Security rule action configuration]


            1. Choose your workflow from the dropdown
            2. Save the rule
For Stack Monitoring or Observability Rules
            1. Navigate to Stack Management → Rules
            2. Find or create your alerting rule
            3. In the Actions section, click Add action
            4. Select Run Workflow


[Screenshot placeholder - Stack rule action configuration]


            1. Select your workflow
            2. Configure when the action should run (e.g., "On alert", "On recovery")
            3. Save the rule
Alert Context Data
When an alert triggers your workflow, it provides rich context data through the event field




Core Concepts - Flow Control Steps
Flow Control
Flow control steps allow you to add logic, conditionals, and loops to your workflows, making them dynamic and responsive to data.
Available Flow Control Steps
            * Conditional Execution (if/else): Execute different steps based on KQL conditions
            * Loops and Iteration (ForEach): Iterate over arrays or collections
            * Execution Control (Wait):  Pause execution for a duration
If/Else Conditions
Execute different workflow paths based on KQL (Kibana Query Language) conditions.
How It Works
The if step type evaluates a KQL condition and executes different steps based on whether the condition is true or false.


steps:
  - name: conditionalStep
    type: if
    condition: <KQL expression> 
    steps:
      # Steps to execute if condition is true (then branch)
    else:
      # Steps to execute if condition is false (optional)
	

Important: Correct Syntax
The if step has a specific structure:
            * condition: KQL expression wrapped in template syntax
            * steps: Array of steps to execute if condition is true (the "then" branch)
            * else: Array of steps to execute if condition is false (optional)
✅ Correct Syntax


- name: checkSeverity
  type: if
  condition: event.severity: 'critical'
  steps:
    - name: handleCritical
      type: console
      with:
        message: "Critical alert!"
  else:
    - name: handleNormal
      type: console
      with:
        message: "Normal severity"
	

❌ Incorrect Syntax


# Wrong - using 'then' instead of 'steps'
- name: wrongSyntax1
  type: if
  condition: "{{ event.severity: 'critical' }}"
  then:  # Should be 'steps'
    - name: step1
      type: console


# Wrong - using 'with' wrapper
- name: wrongSyntax2
  type: if
  with:  # Should not wrap condition in 'with'
    condition: "{{ event.severity: 'critical' }}"
	

National Parks Demo Examples
Check Search Results Count


name: National Parks Conditional Processing
steps:
  - name: searchParks
    type: elasticsearch.search
    with:
      index: national-parks-index[an]
      size: 100
  
  - name: checkResultCount
    type: if
    condition: "steps.searchParks.output.hits.total.value > 5"[ao][ap]
    steps:
      - name: processLargeDataset
        type: foreach
        foreach: "{{ steps.searchParks.output.hits.hits[aq] }}"
        steps:
          - name: processPark
            type: console
            with:
              message: "Processing park: {{ item._source.title[ar] }}"
    else:
      - name: handleSmallDataset
        type: console
        with:
          message: "Only {{ steps.searchParks.output.hits.total.value }} parks found - manual review needed"
	

ForEach Loops
Iterate over arrays or collections to process multiple items with the same steps.
How It Works
The foreach step type iterates over an array, executing a set of steps for each item in the collection.


steps:
  - name: loopStep
    type: foreach
    foreach: <array expression> 
    steps:
      # Steps to execute for each item
      # Current item is available as 'item'
	

Basic Syntax
Required Parameters
            * foreach: The array or collection to iterate over (template expression resolving to a JSON array or workflow context reference).
            * steps: Steps to execute for each item
Important: Item Variable
Inside the loop, the current item is always available as foreach.item. You cannot customize this variable name.


- name: processParks
  type: foreach
  foreach: steps.searchParks.output.hits.hits 
  steps:
    - name: logPark
      type: console
      with:
        message: "Processing {{ foreach.item._source.title }}"  # 'foreach.item' is the current element
	

National Parks Demo Examples
Process Each Park


name: National Parks Enrichment
description: Enrich each park with additional data
steps:
  - name: searchAllParks
    type: elasticsearch.search
    with:
      index: national-parks-index
      size: 100
      query:
        match_all: {}
  
  - name: enrichEachPark
    type: foreach
    foreach: "{{ steps.searchAllParks.output.hits.hits }}"
    steps:
      - name: logProcessing
        type: console
        with:
          message: "Processing park: {{ foreach.item._source.title }}"
      
      - name: addMetadata
        type: elasticsearch.update
        with:
          index: national-parks-index
          id: "{{ foreach.item._id }}"
          doc:
            last_processed: "{{ execution.startedAt }}"
            workflow_run: "{{ execution.id }}"
            category_uppercase: "{{ foreach.item._source.category | upcase }}"
	

Wait Steps
Pause workflow execution for a specified duration.
How It Works
The wait step type pauses the workflow execution for a specified duration before continuing to the next step.


steps:
  - name: waitStep
    type: wait
    with:
      duration: "5s"  # Wait for 5 seconds
	

Duration Format
The duration uses a specific format with units in descending order:
            * w - weeks
            * d - days
            * h - hours
            * m - minutes
            * s - seconds
            * ms - milliseconds
Format Rules
            * Units must be in descending order: 1w2d3h4m5s6ms
            * Each unit can only appear once
            * No spaces between number and unit
            * Integer values only (no decimals)
✅ Valid Examples


duration: "5s"           # 5 seconds
duration: "30s"          # 30 seconds
duration: "2m"           # 2 minutes
duration: "5m30s"        # 5 minutes 30 seconds
duration: "1h"           # 1 hour
duration: "1h30m"        # 1 hour 30 minutes
duration: "1d"           # 1 day
duration: "2d12h"        # 2 days 12 hours
duration: "1w"           # 1 week
duration: "1w3d5h20m10s" # Complex duration
duration: "500ms"        # 500 milliseconds
duration: "2s500ms"      # 2.5 seconds
	

❌ Invalid Examples


duration: "5"            # Missing unit
duration: "5 s"          # Space between number and unit
duration: "1.5s"         # Decimal not allowed
duration: "1,000s"       # Comma not allowed
duration: "5ss"          # Duplicate unit
duration: "10m5s2m"      # Units not in order
duration: "invalid"      # Not a valid format
duration: "-5s"          # Negative not allowed
	



Core Concepts - Action Steps
Action Steps
Action steps are the building blocks of your workflows that perform tasks. They are the "verbs" that do the work, whether it's searching data, calling an API, or sending a notification.


There are two main categories of action steps in Elastic Workflows:
1. Internal Actions
Internal actions are built-in steps that provide native integration with the Elastic Stack. They are the easiest and most secure way to interact with Elasticsearch and Kibana APIs.


These actions are automatically authenticated and provide a simplified interface for common operations.
            * Elasticsearch Actions: Directly interact with the Elasticsearch REST APIs for searching, indexing, deleting, and managing data.
            * Learn more about Elasticsearch Actions
            * Kibana Actions: Interact with Kibana APIs for tasks like creating cases, managing alerts, or interacting with other Kibana features.
            * Learn more about Kibana Actions
2. External Actions (Connectors)
External actions allow your workflows to communicate with third-party systems. Use them to send notifications, create issues in external systems, or call any external API.
            * Examples:
            * servicenow.create_incident
            * slack
            * jira.create_issue
            * http


Next Steps:
            * Dive into Internal Elasticsearch Actions to learn how to manipulate your data.
            * Explore Internal Kibana Actions to integrate with Kibana services.
            * Explore external actions




Kibana
Internal Actions: Kibana
Kibana actions allow your workflows to interact with Kibana's own APIs. This enables you to automate tasks like creating cases, updating alerts, or interacting with other Kibana solutions.


Like Elasticsearch actions, all Kibana actions are automatically authenticated using the permissions of the user or API key executing the workflow.


There are two ways to use Kibana actions: Named Actions and the Generic Request Action.




	1. Named Actions
Named actions provide a simplified, high-level interface for common Kibana operations. These are the preferred method for interacting with Kibana as they provide a clear and stable contract.


The action type corresponds to a specific, curated Kibana function.
Example: Creating a Case
This step uses the kibana.createCaseDefaultSpace action to open a new security case. The parameters in the with block are specific to this action.


- name: create_a_case
  type: kibana.createCaseDefaultSpace
  with:
    title: "Suspicious Login Detected"
    description: "Automated case created by workflow 'Suspicious Login Playbook'. Host '{{ event.host.name }}' exhibited unusual activity."
    tags: ["workflow", "automated-response"]
    severity: "critical"
    connector:
      id: "none"
      name: "none"
      type: ".none"
	Note: The list of available named Kibana actions is evolving. A comprehensive list will be provided as more actions are added.




	2. Generic Request Action (kibana.request)
For interacting with any Kibana API endpoint that does not have a named action, you can use the generic kibana.request. This provides maximum flexibility by allowing you to construct a raw HTTP request to any Kibana API.
Syntax


Parameter
	Description
	Required
	method
	The HTTP method (e.g., POST, PUT, GET).
	No (defaults to GET)
	path
	The API endpoint path, starting with /api/ or /internal/.
	Yes
	body
	The JSON request body.
	No
	query
	An object representing URL query string parameters.
	No
	headers
	Custom HTTP headers to include in the request. kbn-xsrf and Content-Type are added automatically.
	No
	

Example: Unisolate an Endpoint
This example uses the generic request to call the Endpoint Security API to unisolate a host. This is useful for actions that are highly specific and may not have a dedicated named action.


- name: unisolate_endpoint_with_case
  type: kibana.request
  with:
    method: POST
    path: /api/endpoint/action/unisolate
    body:
      endpoints: 
        - "{{ endpoint_id_value }}"
      comment: "Unisolating endpoint as part of automated cleanup."
	

Important Security Note on Authorization
In the example above, you might be tempted to pass an Authorization header. This is not necessary. The workflow engine automatically attaches the correct authentication headers to the request based on the execution context. You should not manage or pass API keys or secrets in the headers block for kibana.request steps.




	Choosing Between Named and Generic Actions
            * Use Named Actions whenever possible. They are simpler, more readable, and provide a more stable interface that is less likely to break with Kibana updates.
            * Use the Generic kibana.request for:
            * Interacting with Kibana APIs that do not yet have a named action.
            * Advanced use cases that require specific headers or query parameters not exposed by a named action.




Elasticsearch
Internal Actions: Elasticsearch
Elasticsearch actions are built-in steps that allow your workflows to interact directly with the Elasticsearch APIs. You can search, index, update, and delete documents, manage indices, and perform any other operation supported by the Elasticsearch REST API.


All Elasticsearch actions are automatically authenticated using the permissions of the user or API key executing the workflow.


There are two ways to use Elasticsearch actions: Named Actions and the Generic Request Action.




	1. Named Actions
Named actions provide a convenient, structured way to call specific Elasticsearch endpoints. The action type directly maps to the Elasticsearch API.
            * elasticsearch.search -> POST /<index>/_search
            * elasticsearch.delete -> DELETE /<index>/_doc/<id>
            * elasticsearch.indices.create -> PUT /<index>


The parameters you provide in the with block are passed as the body or query parameters of the API request.
Example: Searching for Documents
This step searches for documents in the specified index. The query and sort parameters are passed directly to the Search API.


- name: search_for_alerts
  type: elasticsearch.search
  with:
    index: ".alerts-security.attack.discovery*"
    query:
      bool:
        filter:
          - term:
              kibana.alert.severity: "critical"
	

Example: Deleting a Document
This step deletes a single document by its ID. The index and id are used to construct the API path.


- name: delete_a_doc
  type: elasticsearch.delete
  with:
    index: "my-index"
    id: "document_id_123"
	

Example: Bulk Indexing
The elasticsearch.bulk action is used to perform multiple indexing or delete operations in a single request. The body parameter must be a string containing the bulk operations in newline-delimited JSON (NDJSON) format. Each operation requires an action/metadata line followed by an optional source document line.


- name: bulk_index_data
  type: elasticsearch.bulk
  with:
    index: "national-parks-data"
    body: |
      { "index": { "_id": "1" } }
      { "name": "Yellowstone National Park", "category": "geothermal" }
      { "index": { "_id": "2" } }
      { "name": "Grand Canyon National Park", "category": "canyon" }      
	



	2. Generic Request Action (elasticsearch.request)
For advanced use cases or for accessing Elasticsearch APIs that may not have a named action, you can use the generic elasticsearch.request type. This gives you full control over the HTTP request.
Syntax


Parameter
	Description
	Required
	method
	The HTTP method (e.g., GET, POST, PUT, DELETE).
	No (defaults to GET)
	path
	The API endpoint path (e.g., /_search, /_cluster/health).
	Yes
	body
	The JSON request body.
	No
	query
	An object representing URL query string parameters.
	No
	

Example: Getting Cluster Health
This example calls the GET /_cluster/health endpoint.


- name: get_cluster_health
  type: elasticsearch.request
  with:
    method: GET
    path: /_cluster/health
	

Example: Deleting Documents with _delete_by_query
This example uses the generic request to call the _delete_by_query API, which might not have a dedicated named action.


- name: delete_old_documents
  type: elasticsearch.request
  with:
    method: POST
    path: /my-index/_delete_by_query
    body:
      query:
        range:
          "@timestamp":
            lt: "now-30d"
	



	Combining Actions: A Practical Example
The following example demonstrates a common pattern: searching for documents and then iterating over the results to delete each one.


name: Search and Delete Documents
triggers:
  - type: manual
steps:
  - name: search_for_docs
    type: elasticsearch.search
    with:
      index: ".alerts-security.attack.discovery.alerts-default"
      query:
        term:
          host.name: "compromised-host"


  - name: delete_found_docs
    type: foreach
    # The search results are in steps.search_for_docs.output
    foreach: steps.search_for_docs.output.hits.hits
    steps:
      - name: delete_each_doc
        type: elasticsearch.delete
        with:
          # The 'item' variable holds the current document from the loop
          index: "{{ item._index }}"
          id: "{{ item._id }}"
	

Key Concepts in this Example:
            * Data Flow: The output of the search_for_docs step is available to subsequent steps at steps.search_for_docs.output.
            * foreach Loop: The foreach step iterates over the hits.hits array from the search results.
            * item Variable: Inside the loop, the special item variable holds the current document being processed, allowing you to access its fields like item._index and item._id.




External Actions
External Actions
This document covers actions that interact with systems outside of Elastic, as well as the native, built-in http action.
1. Connector-Based Actions (e.g., Slack, Jira)
Connector-based actions leverage Kibana's centralized Connectors framework. To use them, you must first configure a connector in Stack Management > Connectors.


The step type is a simple keyword for the service (e.g., slack), and you must provide a connector-id at the same level as type.
Identifying a Connector
The connector-id field is flexible. You can use either:
            * The unique name you gave the connector (e.g., "my-slack-connector"). This is the recommended method for readability.
            * The connector's raw ID (e.g., "d6b62e80-ff9b-11ee-8678-0f2b2c0c3c68").


The system will first try to find a connector by ID; if that fails, it will search by name.
Example: Sending a Slack Notification
This example uses a pre-configured Slack connector named "security-alerts-channel".


- name: notify_security_channel
  type: slack
  connector-id: "security-alerts-channel"
  with:
    message: "High-priority alert: {{ event.name }}. Please investigate immediately."
	

Example: Creating a Jira Issue
This example uses a Jira connector uniquely named "engineering-project".


- name: create_jira_ticket
  type: jira
  connector-id: "engineering-project"
  with:
    projectKey: "ENG"
    issueType: "Task"
    summary: "Automated Task: Review '{{ event.name }}'"
    description: "Workflow '{{ workflow.name }}' requires manual review for a potential issue."
	



	2. Native http Action
Workflows include a native http action that acts as a simple, built-in HTTP client. It does not require a pre-configured connector and is ideal for one-off requests to public or internal APIs.
Syntax


Parameter
	Description
	Required
	url
	The full URL of the endpoint to call.
	Yes
	method
	The HTTP method (e.g., GET, POST, PUT).
	No (defaults to GET)
	headers
	An object of key-value pairs for HTTP headers.
	No
	body
	The request body (typically a JSON object).
	No
	

Important Note on Authentication


Limitation: The native http action does not currently have access to a centralized secret store for managing authentication credentials (like API keys or tokens).


This means that if your endpoint requires authentication, you must include the credentials directly in the headers block in plain text. We are aware of this limitation and are actively working to provide a more secure solution in a future release.


Example (with plaintext token):


> - name: call_secure_api
>   type: http
>   with:
>     url: "https://api.thirdparty.com/v1/data"
>     method: "GET"
>     headers:
>       Authorization: "Bearer my-super-secret-api-token"
> ```


### Example: Calling a Custom Webhook


This example makes a POST request to a custom automation endpoint, passing data from the workflow context.


```yaml
- name: trigger_custom_automation
  type: http
  with:
    url: "https://hooks.example.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
    method: "POST"
    headers:
      Content-Type: "application/json"
    body:
      event_id: "{{ event.id }}"
      message: "Workflow action triggered by '{{ workflow.name }}'"
	

	

	



If
If Step
The if step provides conditional execution of workflow steps based on boolean expressions or KQL (Kibana Query Language) conditions.
Syntax
steps:
  - name: check-status
    type: if
    condition: "{{ steps.getData.output.status }}: active"
    steps:
      ...
    else:
      ...
Configuration
            * name: Unique step identifier
            * type: Must be "if"
            * condition: Boolean expression or KQL condition string (required)
            * steps: Array of steps to execute when the condition evaluates to true (minimum 1 step, required)
            * else: Array of steps to execute when the condition evaluates to false (optional)
Condition Evaluation Flow
The if step evaluates conditions according to a defined priority order. The evaluation process determines which branch (steps or else) is executed.
Evaluation Priority
The condition is evaluated in the following order:
            1. Template Rendering (always first)
            2. Boolean Evaluation (if boolean)
            3. KQL Evaluation (if string)
            4. Default Handling (if undefined)
Detailed Flow
Step 1: Template Rendering
The condition is first rendered through the templating engine
What happens:
            * Template expressions such as {{ }} or ${{ }} are evaluated.
            * Variables are replaced with their actual values.
            * The result can be a boolean, string, or other type.
Step 2: Type Check and Evaluation
After rendering, the condition is evaluated based on its resulting type.
Boolean Result (Priority 1)
If the rendered condition is a boolean, it is used directly.
Use when: The ${{ }} syntax evaluates to a boolean value.
Undefined Result (Priority 2)
If the rendered condition is undefined, it defaults to false.
String Result (Priority 3) - KQL Evaluation
If the rendered condition is a string, it is evaluated as a KQL expression.
Use when: The condition is written as a plain string or a template expression that renders to a string.
condition: "status: active"  # KQL expression
condition: "{{ steps.getData.output.status }}: active"  # Template → KQL
Invalid Type (Error)
If the rendered condition is neither a boolean, string, nor undefined, an error is thrown.
Causes errors: Objects, arrays, numbers, or other non-boolean/non-string types.
Complete Flow Diagram
Condition String/Expression
         ↓
[1] Template Rendering (renderValueAccordingToContext)
         ↓
    Rendered Result
         ↓
[2] Type Check
         ↓
    ┌──┴──┬──────┬──────┐
    │         │          │         │
Boolean   String    Undefined   Other
    │         │          │         │
    │         │          │         │
[3] Return  [4] KQL    [5] Return  [6] Error
  Value    Evaluation    false
    │         │          │
    └─────────┴─────────┘
         ↓
    Final Boolean
         ↓
    Branch Selection
Condition Types
The condition field supports two primary expression types.
1. Boolean Expression (${{ }})
When using ${{ }} syntax, the expression must evaluate to a boolean value.
Behavior:
            * The expression is evaluated and must return true or false.
            * undefined values default to false.
            * Other types (objects, arrays, numbers) cause an error.
2. KQL Expression (String)
When using string-based conditions (with or without {{ }} templating), the value is evaluated as a KQL expression.
The condition string is first rendered as a template, then interpreted as KQL.
KQL Condition Syntax
The if step supports a subset of KQL (Kibana Query Language) for evaluating conditions.
Equality
condition: "status: active"
condition: "user.role: admin"
condition: "isActive: true"
condition: "count: 42"
condition: "users[0].name: Alice"  # Array index access
Range Operators
condition: "count >= 100"
condition: "count <= 1000"
condition: "count > 50"
condition: "count < 200"
condition: "count >= 100 and count <= 1000"  # Combined
Wildcard Matching
condition: "fieldName:*"        # Field exists
condition: "user.name: John*"   # Starts with
condition: "user.name: *Doe"    # Ends with
condition: "txt: *ipsum*"       # Contains
condition: "user.name: J*n Doe" # Pattern
Logical Operators
condition: "status: active and isEnabled: true"             # AND
condition: "status: active or status: pending"              # OR
condition: "not status: inactive"                           # NOT
condition: "status: active and (role: admin or role: moderator)"  # Nested
Property Path Access
condition: "user.info.name: John Doe"            # Nested property
condition: "steps.fetchData.output.status: completed"  # Deep nesting
condition: "users[0].name: Alice"                # Array access
condition: "users.0.name: Alice"                 # Alternative syntax
Examples
Boolean Expression
steps:
  - name: check-enabled
    type: if
    condition: "${{ inputs.isEnabled }}"
    steps:
      - name: process-enabled
        type: http
    else:
      - name: log-disabled
        type: console
KQL with Templating
steps:
  - name: check-status
    type: if
    condition: "{{ steps.fetchData.output.status }}: completed"
    steps:
      - name: process-data
        type: http
Complex KQL
steps:
  - name: check-complex
    type: if
    condition: "status: active and (count >= 100 or role: admin)"
    steps:
      - name: process-authorized
        type: http
Best Practices
            1. Use boolean expressions for simple checks.
 Prefer ${{ }} syntax when the result is a boolean.
            2. Use KQL for more complex conditions.
 KQL allows flexible comparison and logical operators.
            3. Template dynamic values when needed.
 Use {{ }} within KQL to inject data into the condition.
            4. Handle both branches.
 Include else when both true and false paths require handling.
            5. Keep conditions readable.
 Avoid overly complex expressions that are difficult to debug.
            6. Test edge cases.
 Verify conditions against true, false, undefined, and missing fields.
Error Scenarios
Invalid KQL Syntax
condition: "invalid""condition"  # Syntax error
Error: Syntax error in condition for step.
Invalid Condition Type
condition: "${{ inputs.config }}"  # Returns object, not boolean
Error: Invalid condition type – expected boolean or string.
Missing Field in KQL
condition: "nonExistentField: value"
Behavior: Returns false (the field does not exist), no error is thrown.
Supported KQL Features
Supported
               * Equality checks and range operators (>=, <=, >, <)
               * Wildcard matching (*, field:*)
               * Logical operators (and, or, not)
               * Nested property access and array index access
               * Boolean, string, and number comparisons
Not Supported
               * Full-text search, regex patterns, or complex aggregations
               * Date/time functions or mathematical operations in conditions
On Failure


On-Failure Configuration
The on-failure configuration defines error-handling behavior for workflow steps.
It can be configured either at the step level or at the workflow level (under settings).
1. Configuration Levels
1.1 Step-Level Configuration
steps:
  - name: api-call
    type: http
    on-failure:
      retry:
        max-attempts: 3
        delay: "5s"
1.2 Workflow-Level Configuration
settings:
  on-failure:
    retry:
      max-attempts: 2
      delay: "1s"
steps:
  - name: api-call
    type: http
Note:
 Step-level on-failure configuration always overrides workflow-level settings.
2. Options
2.1 Retry
Retries the failed step a configurable number of times, optionally with a delay between attempts.
on-failure:
  retry:
    max-attempts: 3  # Required, minimum 1
    delay: "5s"      # Optional, duration format (e.g., "5s", "1m", "2h")
Behavior:
               * Retries up to max-attempts times.
               * Waits for delay between retries, if specified.
               * Workflow enters the WAITING state if the delay exceeds the task manager threshold.
               * Workflow fails if all retries are exhausted.
2.2 Fallback
Executes alternative steps after the primary step fails and all retries (if any) are exhausted.
on-failure:
  fallback:
    - name: fallback-step
      type: http
      with:
        url: "https://backup-api.example.com"
Behavior:
               * Executes fallback steps only after all retries have failed (if retry is configured).
               * The original error is preserved in the workflow execution.
               * Workflow continues after executing the fallback steps.
2.3 Continue
Continues workflow execution even if a step fails.
on-failure:
  continue: true
Behavior:
               * Workflow proceeds to the next step even if the current step fails.
               * The step failure is recorded, but it does not interrupt workflow execution.
               * The workflow completes successfully despite the error.
3. Processing Order
on-failure options are applied in the following order:
retry → fallback → continue
               1. Retry (if configured): Wraps the step with retry logic.
               2. Fallback (if configured): Executes fallback steps after retries are exhausted.
               3. Continue (if configured): Ensures workflow continues regardless of failure outcome.
4. Combining Options
Multiple failure-handling options can be combined in a single configuration.
on-failure:
  retry:
    max-attempts: 2
    delay: "1s"
  fallback:
    - name: backup-step
      type: http
  continue: true
Execution Flow:
 Retry → Fallback → Continue
5. Restrictions
               * Flow-control steps (if, foreach) cannot have workflow-level on-failure configurations.
               * Step-level on-failure definitions are removed during graph construction to prevent recursive execution.
               * Fallback steps execute only after all retries have been exhausted.
Wait
Wait Step
The wait step pauses workflow execution for a specified duration.
Syntax
steps:
  - name: wait-before-retry
    type: wait
    with:
      duration: "5s"
Configuration
               * type — Must be "wait".
               * with.duration — Duration string (required).
Duration Format
Duration strings must follow the format "1w2d3h4m5s6ms" using descending time units.
Supported Units
               * w — weeks
               * d — days
               * h — hours
               * m — minutes
               * s — seconds
               * ms — milliseconds
Examples
duration: "5s"        # 5 seconds
duration: "1m"        # 1 minute
duration: "2h30m"     # 2 hours 30 minutes
duration: "1w2d3h"    # 1 week, 2 days, 3 hours
duration: "1h39s"     # 1 hour 39 seconds
Invalid Formats
               * "5ss", "10m5ss" — Duplicate units
               * "1s1w", "2h1d" — Incorrect order (must be descending)
               * "-1s", "0" — Negative or zero values
               * "1.5s", "1,000s" — Decimals or commas are not allowed
Execution Modes
Short Duration (≤ 5 seconds)
For durations of 5 seconds or less, the wait is handled synchronously using setTimeout.
Behavior
               * Waits in-memory using setTimeout
               * Can be aborted via the abort controller
               * Workflow remains in RUNNING state
               * Execution continues immediately after the wait completes
Example
steps:
  - name: short-wait
    type: wait
    with:
      duration: "3s"
Long Duration (> 5 seconds)
For durations longer than 5 seconds, the step schedules a resume task via the task manager.
Behavior
               * Schedules a resume task for future execution
               * Workflow enters WAITING state
               * Step state stores resumeExecutionTaskId
               * Execution resumes when the scheduled task runs
Entering Long Wait
               1. Start step execution
               2. Schedule resume task for currentTime + duration
               3. Store resumeExecutionTaskId in step state
               4. Set workflow to WAITING state
Exiting Long Wait
               1. Clear step state
               2. Finish step execution
               3. Move to the next node
Example
steps:
  - name: long-wait
    type: wait
    with:
      duration: "1h"
Abort Behavior
Short-duration waits can be aborted via the abort controller.
When aborted:
               * Wait is cancelled immediately
               * Step execution does not finish
               * Workflow does not continue to the next node
               * Error: "Wait step was aborted"
Long-duration waits cannot be aborted once scheduled.
Examples
Simple Wait
steps:
  - name: delay
    type: wait
    with:
      duration: "10s"
Wait Before Retry
steps:
  - name: api-call
    type: http
    on-failure:
      retry:
        max-attempts: 3
        delay: "5s"


  - name: wait-before-next
    type: wait
    with:
      duration: "1m"
Long Wait
steps:
  - name: wait-one-day
    type: wait
    with:
      duration: "1d"


Implementation Details
               * Threshold: SHORT_DURATION_THRESHOLD = 5000ms (5 seconds)
               * Duration Parsing: parse-duration.ts
               * Task Manager: Long waits use workflowTaskManager.scheduleResumeTask() for scheduling
               * State Management: Long waits store resumeExecutionTaskId in step state for tracking


Foreach
Foreach Step
The foreach step iterates over an array and executes its nested steps once for each item in the array.
1. Configuration
               * name — Unique step identifier.
               * type — Must be "foreach".
               * foreach — Expression that evaluates to an array (required).
               * steps — Array of steps to execute for each iteration (minimum 1 step, required).
2. Foreach Expression Evaluation Flow
The foreach expression is evaluated using a priority-based flow to determine the array to iterate over.
2.1 Evaluation Priority
               1. Template Expression Detection — Check if the expression starts with {{ and ends with }}.
               2. Expression Evaluation (if template expression) OR Template Rendering (if not).
               3. JSON Parsing (if the result is a string).
               4. Array Validation (final check).
2.2 Detailed Flow
Step 1: Template Expression Detection
Path A: Expression Evaluation ({{ }} or ${{ }})
               * If the expression starts with {{ or ${{ and ends with }}, it is evaluated as an expression.
               * Behavior:
               * Preserves the type of the result.
               * Result can be:
               * Array — used directly.
               * String — JSON-parsed.
               * Other types — throws an error.
               * Both {{ }} and ${{ }} syntaxes behave identically for foreach.
Path B: Template Rendering
               * If the expression is not a template expression, it is rendered as a template string.
               * Behavior:
               * Any embedded {{ }} templates are evaluated.
               * The result is always a string.
               * String is parsed as JSON to produce an array.
Examples:
# Direct JSON string
foreach: '["item1", "item2", "item3"]'
# → String → JSON parse → Array


# JSON string with embedded templates
foreach: '[{{ steps.getCount }}, {{ steps.getCount | plus: 1 }}]'
# → Rendered string → JSON parse → Array
Step 2: JSON Parsing (if string result)
               * Strings produced from rendering are parsed into arrays using JSON parsing.
               * Parsing errors result in workflow failure.
Step 3: Array Validation
               * The final result must be an array.
               * Non-array results throw a validation error.
2.3 Flow Diagram
Foreach Expression
         ↓
[1] Check: starts with '{{' and ends with '}}'?
         ↓
    ┌──┴──┐
    │        │
   Yes       No
    │        │
    ↓         ↓
[2A] Expression    [2B] Template
     Evaluation        Rendering
         ↓                ↓
    Result (any)      String
         │               │
         └───┬─────┘
                ↓
         [3] Is string?
                ↓
         ┌───┴───┐
         │            │
        Yes           No
         │            │
          ↓            │
    [3A] JSON parse    │
         │            │
         └───┬───┘
                ↓
         [4] Array Validation
                ↓
         Final Array
3. Expression Types
3.1 Template Expression ({{ }} or ${{ }})
               * Syntax: Expression wrapped in {{ }} or ${{ }}.
               * Evaluation: Preserves type.
               * Use case: Array comes from context variables (step outputs, inputs, constants).
foreach: "{{ steps.getData.output.items }}"
foreach: "${{ steps.getData.output.items }}"  # Same behavior
3.2 JSON String
               * Syntax: Plain JSON array string.
               * Evaluation: Template rendering → JSON parse.
               * Use case: Static arrays known at definition time.
foreach: '["item1", "item2", "item3"]'
3.3 JSON String with Templates
               * Syntax: JSON string containing {{ }} template expressions.
               * Evaluation: Template rendering → Evaluate templates → JSON parse.
               * Use case: Dynamically built arrays with known structure
foreach: '[{{ steps.getCount }}, {{ steps.getCount | plus: 1 }}]'
3.4 Direct Property Path (Not Recommended)
               * Syntax: Plain property path without template syntax.
               * Evaluation: Template rendering → JSON parse (may fail).
               * Recommendation: Avoid; prefer {{ consts.items }}.
foreach: 'consts.items'        # Not recommended
foreach: "{{ consts.items }}"   # Preferred
4. Context Variables
During foreach iteration, the following context variables are available:
               * foreach.item — Current item in the iteration.
               * foreach.index — Zero-based index of the current iteration.
               * foreach.total — Total number of items in the array.
               * foreach.items — Complete array being iterated over.
Example:
message: "Processing {{ foreach.item.name }} ({{ foreach.index | plus: 1 }}/{{ foreach.total }})"
4.1 Accessing Parent Foreach Context
Nested foreach loops can access parent context using step references:
steps:
  - name: outer-foreach
    type: foreach
    foreach: "{{ outerItems }}"
    steps:
      - name: inner-foreach
        type: foreach
        foreach: "{{ innerItems }}"
        steps:
          - name: log-both
            type: console
            with:
              message: "Outer: {{ steps.outer-foreach.index }}, Inner: {{ foreach.index }}"


4.2 Accessing Keys with dots:
Template expressions support bracket notation for keys that contain dots or other special characters.
"{{ foreach.item['service.name'] }}"
5. Execution Flow
               1. Initialization: Evaluate foreach expression. Skip step if array is empty.
               2. Iteration: Enter isolated scope → execute nested steps → advance to next item.
               3. Completion: Exit foreach and continue to the next workflow step.
6. Error Scenarios
               * Non-array result: Throws error with the expression and resolved type.
               * Invalid JSON: Throws parsing error with the invalid string.
               * Missing expression: Throws error requiring foreach configuration.
Core Concepts - Data & Error Handling
Data and Error Handling
A key feature of Elastic Workflows is the ability to pass data between steps and to handle failures gracefully. This document explains the mechanisms for controlling data flow and building resilient, fault-tolerant automations.
Data Flow: Passing Data Between Steps
Every step in a workflow produces an output. By default, this output is added to a global steps object in the workflow's context, making it available to all subsequent steps.


You can access the output of a specific step using the following syntax:


steps.<step_name>.output


Additionally, you can access any error information from a step:


steps.<step_name>.error
Example: Chaining Steps with Data
This workflow demonstrates a common pattern: searching for data in one step and using the results in a later step.
               1. The first step, find_user_by_id, searches an index for a document.
               2. The second step, create_case_for_user, uses the output of the first step to enrich a new case. Notice how the description field accesses steps.find_user_by_id.output.hits.hits[0]._source.user.fullName.


name: Create Case for a Specific User
steps:
  - name: find_user_by_id
    type: elasticsearch.search
    with:
      index: "my-user-index"
      query:
        term:
          user.id: "u-123"


  - name: create_case_for_user
    type: kibana.createCaseDefaultSpace
    with:
      title: "Investigate user u-123"
      description: "A case has been opened for user {{ steps.find_user_by_id.output.hits.hits[0]._source.user.fullName }}."
      tags: ["user-investigation"]
      connector:
        id: "none"
        name: "none"
        type: ".none"
	



	Error Handling
By default, if any step in a workflow fails, the entire workflow execution stops immediately. However, you can override this behavior and define custom error handling logic using the on-failure block.


The on-failure block is a special property you can add to any step. It contains a fallback array of steps to execute only if the primary step fails.
On-Failure Error Handling
When a step fails, you can define recovery actions using the on-failure configuration:


steps:
  - name: risky_operation
    type: elasticsearch.search
    with:
      index: "non-existent-index"
      query:
        match_all: {}
    on-failure:
      fallback:
        - name: log_error
          type: console
          with:
            message: "Operation failed, using fallback"
        - name: default_response
          type: http
          with:
            method: GET
            url: "https://api.example.com/default"
	Within the on-failure.fallback steps, you can access the error information of the failed step using:


steps.<failed_step_name>.error
Example: Handling Elasticsearch Failures
This workflow attempts to delete a document. If the elasticsearch.delete action fails, the on-failure block executes alternative steps:


- name: delete_critical_document
  type: elasticsearch.delete
  with:
    index: "my-critical-index"
    id: "doc-abc-123"
  on-failure:
    fallback:
      - name: notify_on_failure
        type: slack
        connector-id: "devops-alerts"
        with:
          message: "Failed to delete document in workflow '{{ workflow.name }}'"
      - name: log_failure
        type: console
        with:
          message: "Document deletion failed, error: {{ steps.delete_critical_document.error }}"
       
	

Example: Continuing After Failure
Sometimes, a failure is not critical, and you want the workflow to continue. The on-failure.fallback mechanism allows you to provide alternative execution paths when the primary operation fails.


- name: create_ticket
  type: jira
  connector-id: "my-jira-project"
  with:
    projectKey: "PROJ"
    summary: "New issue from workflow"
  On-failure:
    continue: true
    fallback:
      - name: notify_jira_failure
        type: slack
        connector-id: "devops-alerts"
        with:
          message: "Warning: Failed to create Jira ticket. Continuing workflow."

	   
	

By combining data flow and robust error handling, you can build complex, reliable automations that can react to dynamic conditions and recover from unexpected failures.




Workflow Authoring (YAML Editor) (to do)


Workflow Management (to do)


Use Cases (to do)


Templating Engine


Overview
Workflow Templating Engine
The Workflow Templating Engine enables powerful and type-safe template rendering for workflows using the Liquid templating language.
It supports string interpolation, recursive object rendering, advanced expressions, and type-preserving syntax extensions.
1. Syntax Overview
1.1 Simple String Templates ({{ }})
Use {{ }} for string interpolation. Variables and expressions inside the braces are evaluated and rendered as strings.
message: "Hello {{ user.name }}!" # Result: "Hello Alice"
url: "https://api.example.com/users/{{ user.id }}" # Result: "https://api.example.com/users/12"
1.2 Escaping Template Syntax
Use {% raw %} and {% endraw %} to output literal {{ }} characters without rendering them.
# Output literal {{ }} in rendered text
value: "{% raw %}{{ _ingest.timestamp }}{% endraw %}" # Result: "{{ _ingest.timestamp }}"
1.3 Object and Array Templates
Templates can appear anywhere in nested objects or arrays.
The engine processes all string values recursively.
with:
  headers:
    Authorization: "Bearer {{ token }}"
  tags:
    - "{{ tag1 }}"
    - "{{ tag2 }}"
1.4 Expression Evaluation (${{ }})
Use ${{ }} when you want to preserve the original data type (array, object, number, boolean) instead of converting the result to a string.
# Using {{ }} - converts to string
tags: "{{ inputs.tags }}" # Result: string representation


# Using ${{ }} - preserves type
tags: "${{ inputs.tags }}" # Result: actual array
Important:
 ${{ }} must occupy the entire string value.
✅ Valid:
tags: "${{ inputs.tags }}"
items: "${{ inputs.items | slice: 0, 2 }}"
❌ Invalid:
message: "Tags are: ${{ inputs.tags }}"
1.5 Liquid Tags ({% %})
Use {% %} for control flow and logic, such as conditionals and loops.
Common tags: {% if %}, {% for %}, {% assign %}, {% case %}.
message: |
  {% if user.role == 'admin' %}
    Welcome, administrator!
  {% endif %}
1.6 Liquid Code Blocks ({%- liquid ... -%})
Combine multiple Liquid statements inside one tag block.
message: |
  {%- liquid
    assign greeting = "Hello"
    echo greeting
    echo " "
    echo user.name
  -%}
2. Template Rendering Behavior
The engine renders templates recursively through all data structures, ensuring full support for nested workflows and dynamic data substitution.
2.1 Recursive Rendering
The engine traverses objects and arrays, rendering all string values.
Input:
message: "Hello {{ user.name }}"
config:
  url: "{{ api.url }}"
tags: ["{{ tag1 }}", "{{ tag2 }}"]
After Rendering:
message: "Hello Alice"
config:
  url: "https://api.example.com"
tags: ["admin", "user"]
2.2 Type Handling
Type
	Behavior
	Strings
	Processed as templates; variables interpolated, filters applied
	Numbers, Booleans, Null
	Returned as-is
	Arrays
	Each element processed recursively
	Objects
	Each property value processed recursively (keys are not processed)
	2.3 ${{ }} vs {{ }} Comparison
Feature
	{{ }}
	${{ }}
	Output Type
	Always string
	Preserves original type
	Arrays
	Stringified
	Actual array
	Objects
	Stringified
	Actual object
	Booleans
	"true" / "false"
	true / false
	Numbers
	"123"
	123
	Filters
	Applied (stringified result)
	Applied (type preserved)
	2.4 Null and Undefined Handling
Case
	Behavior
	Null values
	Returned as-is
	Undefined variables
	Empty string in {{ }}; undefined in ${{ }}
	Missing context properties
	Treated as undefined
	2.5 Filter Application
Filters are applied during rendering and can be chained together.
name: "{{ user.name | upcase }}"
data: "{{ jsonString | json_parse | json }}"
3. Summary
The Workflow Templating Engine provides:
✅ Full Liquid syntax support
✅ Recursive rendering for complex structures
✅ Type-preserving evaluation with ${{ }}
✅ Compatibility with Liquid filters and tags
✅ Safe handling for null and undefined values
4. References
               * Liquid Templating Language
               * LiquidJS Documentation
               * LiquidJS GitHub Repository
Templating Languages Evaluation


Templating Language Evaluation
Note: this evaluation was completed with the assistance of Gemini AI and information from Stack Overflow.
This evaluation is conducted under the strict assumption that all templates are untrusted and user-provided, prioritizing security against Server-Side Template Injection (SSTI) and Remote Code Execution (RCE).
Security 🛡️
Strict Sandboxing Model (Engine Design)
Evaluation Focus: Does the engine compile user input into executable JavaScript code?
Why This is Critical: Prevents RCE and SSTI. Engines that parse code into a secured Abstract Syntax Tree (AST) and then interpret it are inherently safer than those that compile user input into executable JavaScript code, which is the primary path to server compromise.
Nunjucks:
               * Analysis: Compiles templates directly into executable JavaScript code. This architecture means the template engine's sandbox is fragile and relies on the js process to block dangerous functions.
               * Conclusion: High Risk. The reliance on brittle runtime checks makes it unsuitable for untrusted input without an external security layer (e.g., vm2).
Handlebars:
               * Analysis: Also compiles to executable JavaScript. While logic-less, historical RCE/Prototype Pollution (PP) flaws prove that compilation is a severe attack surface.
               * Conclusion: High Risk. Requires constant patching and monitoring. Its compilation model makes it a poor choice for sandboxing.
Mustache:
               * Analysis: Does not compile to complex executable code and its minimalism avoids most features that lead to SSTI.
               * Conclusion: Moderate Risk. Its simple logic-less nature offers an implicit security gain, but it provides no formal sandboxing features to control access.
LiquidJS:
               * Analysis: Strongest model. It parses code into a secure AST and interprets it, avoiding eval() or new Function(). Security is enforced by the interpreter, not by runtime checks on compiled JS.
               * Conclusion: Safest Choice. Built specifically for secure, untrusted user-generated content.
Default Access to Host Language (RCE)
Evaluation Focus: Is access to runtime objects (process, require, global) explicitly blocked?
Why This is Critical: Blocks the primary vector for RCE. Ensures attackers cannot "escape" the template context to access the operating system or application environment.
Nunjucks & Handlebars:
               * Analysis: As JS-compiled engines, a successful SSTI payload can often leverage JavaScript's inherent prototype chain to gain access to Node.js process or require.
               * Conclusion: Vulnerable. Requires the developer to manually wrap the rendering in an isolated process to guarantee protection.
Mustache:
               * Analysis: Cannot execute arbitrary code directly, so accessing host objects is theoretically impossible from the template syntax alone.
               * Conclusion: Strong. Its minimalism is its defense here.
LiquidJS:
               * Analysis: The AST interpreter does not recognize the syntax for js object access. This provides a fundamental block against host language escape attempts.
               * Conclusion: Strongest. Provides an architectural barrier against RCE.
Prototype Pollution (PP) Defense
Evaluation Focus: Does the data-binding mechanism explicitly block access/modification of __proto__ and constructor?
Why This is Critical: Mitigates a high-severity RCE vector. PP is a common flaw used in JS templating engines to hijack an application's core logic and execute code.
Nunjucks:
               * Analysis: Inherits the risk of the underlying JavaScript environment. Highly dependent on using strict JSON inputs and freezing objects on the backend.
               * Conclusion: Weak. Lacks native, reliable PP mitigation within the core engine.
Handlebars:
               * Analysis: Historically vulnerable, but modern versions include explicit runtime checks to block dangerous property access (e.g., __proto__). This requires constant maintenance.
               * Conclusion: Moderate/High Maintenance. The defense is reactive (patches) rather than structural.
Mustache:
               * Analysis: Performs minimal logic and object merging, thus avoiding the primary targets of PP attacks.
               * Conclusion: Strong. Inherently resistant due to its simple data processing model.
LiquidJS:
               * Analysis: The engine deals with data lookups and interpretation, not deep object merging or JS compilation, making it structurally resistant to PP.
               * Conclusion: Strongest. Structural resistance to the vulnerability.
Conclusions
For maximum security against untrusted input, the choice is clear:
               * LiquidJS is the safest choice. Its AST-based parsing and secure-by-design philosophy provide the highest resistance against RCE and Prototype Pollution attacks, making it the most suitable for user-editable content.
               * ⚠️ Nunjucks and Handlebars should be avoided for untrusted template creation due to their foundation in JavaScript compilation, which creates a larger, more exploitable attack surface (SSTI/RCE) that relies on constant patching and often requires an external sandbox for mitigation.
Built-in Functionality 🛠️
Built-in Filters and Functions (Data Transformation Focus)
This criterion measures the native, out-of-the-box tools available for manipulating strings, numbers, and arrays without writing custom JavaScript code.
Nunjucks (The Developer's Toolkit)
               * Filters (Approx. 30+): Highest Quality and Diversity. Inheriting the Jinja2 standard, Nunjucks provides a comprehensive, developer-centric filter set.
               * Diversity: Excellent for technical data manipulation. Includes robust filters for:
               * Collections: sum, reverse, sort, groupby, min/max.
               * Strings/Utilities: int, float, default, urlize, slugify.
               * Quality: Filters are designed for complex chaining (e.g., {{ list | sort(true) | first | upper }}) and are highly performant.
               * Key Tags: {% macro %} (reusable component functions) and {% for %} are powerful tools for logic implementation.
LiquidJS (The Presentation Utility)
               * Filters (Approx. 50+): High Quantity and Excellent Practicality. Based on the Shopify standard.
               * Diversity: High utility for general data and presentation. Includes strong filters for:
               * Math/Collections: append, divided_by, minus, size.
               * Output Control: strip_html, url_encode, date.
               * Quality: Filters are reliable and cover most common array and string transformations needed in a web context, offering excellent developer utility.
               * Key Tags: {% assign %} and standard {% for %}.
Handlebars
               * Filters: None. Handlebars has only 6 built-in Helpers (if, each, etc.), but no native data transformation filters.
               * Diversity/Quality: Weakest. Any data manipulation (e.g., calculating a total, uppercasing a string) requires writing and registering a separate Custom JavaScript Helper. This is a deliberate philosophical choice to keep logic out of the template.
               * Key Tags: Focused purely on flow: {{#if}}, {{#each}}.
Mustache
               * Filters: Zero. The language is designed to be purely logic-less.
               * Diversity/Quality: None. Data must be fully formatted into strings on the server before being passed to the engine. The only way to introduce any dynamic transformation is via a Lambda (a function passed as a data property).
               * Key Tags: Purely structural: {{#section}} (loop/conditional), {{> partial}}.
Variable Assignment (BE Evaluation)
This determines the engine's capability to perform and store the results of computations within the template rendering process.
Option
	Assignment Mechanism
	Conclusion
	Nunjucks
	Strongest. Native {% set name = value %} tag.
	Best for backend computation: Allows you to store complex filter results, perform math, and reuse the value later in the template.
	LiquidJS
	Good. Supports {% assign name = value %}.
	Good Utility: Sufficient for simple assignments but less flexible for complex, multi-line logic blocks than Nunjucks.
	Handlebars
	None. No native assignment.
	Worst: Forces complex state management entirely out of the template and into unwieldy Custom Helper solutions.
	Mustache
	None.
	Worst: Incapable of any state management or internal computation.
	Asynchronous Support
This focuses on the engine's ability to handle I/O operations within a filter or tag without blocking the JS event loop.
Option
	Native Async Support
	Conclusion
	Nunjucks
	Yes. Natively supports async filters and tags.
	Strong: Essential for high-concurrency Node.js SSR where data fetching may happen mid-template.
	LiquidJS
	Yes. Built with native async rendering support.
	Strong: Designed for modern, non-blocking asynchronous operations.
	Handlebars
	No. Primarily synchronous.
	Weak: Requires resolving all Promises and async operations before rendering begins.
	Mustache
	No.
	Weak: Purely synchronous and incapable of I/O.
	Conclusions
For a use case centered on data transformation, Nunjucks and LiquidJS are the only viable candidates due to their extensive filter sets and native support for assignment and asynchronous operations.
               * Nunjucks wins on raw data manipulation power and internal logic flow (Assignment + Macros).
               * LiquidJS provides a robust, production-ready set of transformations with high-quality asynchronous support.
Extendability 🔌
The extendability of these engines is crucial for integrating specialized backend business logic (like complex calculations or I/O) and ensuring that the developer can control the scope of those functions to maintain security and modularity.
Custom Tag/Block Support
Evaluation Focus: Can developers define entirely new, custom control structures beyond simple helpers (e.g., a custom {% cache %} tag)?
Why This is Critical: Allows for deep integration and creation of a Domain-Specific Language. This capability is necessary for integrating specialized backend logic that requires unique markup and unique parsing rules (e.g., creating a custom tag for data fetching).
Nunjucks:
               * Analysis: Strongest. Provides the ability to create full Extensions, which grant control over both the template parser and the runtime logic. This is the most flexible system for defining complex, reusable logic blocks.
               * Conclusion: Best for projects requiring the highest degree of structural and functional customization, but keep the poor security in mind.
Handlebars:
               * Analysis: Moderate. Limited to Block Helpers ({{#block}}...{{/block}}). This is useful for simple content wrappers or iterative logic but lacks the low-level parsing control needed for complex, custom syntax.
               * Conclusion: Sufficient for logic wrappers but not for extending the fundamental syntax of the language.
Mustache:
               * Analysis: Weakest. Cannot define custom tags or blocks. All logic is passed in the data context as a Lambda function (which is not a true extension mechanism).
               * Conclusion: Incapable of extending control flow or custom markup.
LiquidJS:
               * Analysis: Strong. Supports custom Tags and Filters via class extensions. This grants significant control over both the parsing and rendering phases of the custom structure.
               * Conclusion: Excellent support for defining new custom logic elements.
Scope of Extensions (Global vs. Local)
Evaluation Focus: Does the API allow filters/tags to be registered on an isolated environment instance?
Why This is Critical: Promotes security and modularity. It ensures that powerful custom functions are not globally available across all templates by default. Isolation is key for managing security, especially with untrusted input.
Nunjucks:
               * Analysis: Strong. Extensions and filters are tied to a specific Environment instance (new nunjucks.Environment()), making local scoping the standard, easy practice.
               * Conclusion: Provides the necessary isolation for secure, modular development.
Handlebars:
               * Analysis: Weak. Helpers are Global by default (Handlebars.registerHelper). To scope them locally, you must explicitly use Handlebars.create() to instantiate an isolated environment, making isolation a necessary extra step.
               * Conclusion: Default configuration is risky; local scoping is achievable but non-intuitive.
Mustache:
               * Analysis: Strong. Custom logic (Lambdas) is passed in the data context, making it inherently local and scoped only to that specific render operation.
               * Conclusion: Naturally isolated, but lacks a centralized extension API.
LiquidJS:
               * Analysis: Strong. Extensions are tied to the specific Engine instance (new Liquid()), promoting isolation by design and facilitating security management.
Asynchronous Functions (Data Fetching/I/O)
Evaluation Focus: Can the engine natively handle and await I/O operations (like an API call) within a filter or tag?
Why This is Critical: Allows for lazy I/O integration. Enables developers to inject functions that perform necessary I/O (database, API calls) only when the data is requested in the template, optimizing backend performance.
Nunjucks:
               * Analysis: Strong. Filters and tags can be registered as async or using callbacks, with the rendering engine pausing and resuming execution upon Promise resolution.
               * Conclusion: Excellent native support for integrating asynchronous backend logic.
Handlebars:
               * Analysis: No. Handlebars is primarily synchronous. Helper functions cannot pause the rendering process.
               * Conclusion: Not suitable for integrating asynchronous logic mid-render.
Mustache:
               * Analysis: No. Purely synchronous.
               * Conclusion: Cannot integrate asynchronous logic.
LiquidJS:
               * Analysis: Strong. The interpreter is built to natively handle Promises returned from filters or custom tags.
               * Conclusion: Excellent native support for modern, non-blocking asynchronous backend integration.
Adding New Filters and Data Manipulation Operators
Nunjucks:
Mechanism: You add custom logic using Filters or Global Functions tied to the Environment instance.
Feature
	Nunjucks Implementation
	Filter
	env.addFilter('new_filter', function)
	Operator
	env.addFilter('new_operator', function)
	Data Flow
	The function receives the input data as the first argument and returns the manipulated data.
	Conclusion
	Excellent. This is the cleanest, most idiomatic way to add data transformation. It supports asynchronous filters for fetching data and full chaining with built-in filters.
	LiquidJS:
Mechanism: You add custom logic using Filters tied to the Engine instance.
Feature
	LiquidJS Implementation
	Filter
	engine.registerFilter('new_filter', function)
	Operator
	engine.registerFilter('new_operator', function)
	Data Flow
	The function receives the input data as the first argument and returns the processed data.
	Conclusion
	Excellent. Provides a clear, dedicated API for extensions on a locally scoped engine. It also natively supports asynchronous filters, making it ideal for I/O operations.
	Handlebars:
Mechanism: You add custom logic using Helpers. Handlebars does not have a "filter" concept; everything is a helper.
Feature
	Handlebars Implementation
	Filter
	Handlebars.registerHelper('new_filter', function)
	Operator
	Requires a dedicated Block Helper or Custom Logic.
	Data Flow
	The function returns the manipulated data. Because it's an expression, filter chaining is not native or intuitive (requiring nested subexpressions or custom helper logic).
	Conclusion
	Good for Simplicity, Poor for Chaining. It's easy to add functions, but the lack of native filter syntax makes complex data transformations (chaining multiple operations) clumsy.
	Mustache:
Mechanism: You expose native JavaScript functions as Lambdas in the data context.
Feature
	Mustache Implementation
	Filter
	Define a function property (Lambda) in the data object passed to the renderer.
{{#myFunc}} data {{/myFunc}} (The function receives the inner text data and returns the result).
	Operator
	Lambda function.
	Data Flow
	The function receives the raw, unrendered block of template code, executes its logic, and returns a new string.
	Conclusion
	Not an Extension Mechanism. This approach is messy for data manipulation and does not allow you to easily define or use filters in the traditional sense. It's best avoided for complex data processing.
	Conclusions
               * For Maximum Functional Power: Nunjucks is the best choice. Its combination of a powerful Custom Tag system, dedicated Filters, and native Async support makes it ideal for complex backend features and data manipulation logic.
               * For Secure, Modular Extendability: LiquidJS is the superior alternative. It matches Nunjucks' strong features in native Async support and Filter APIs, but wraps them in a safer, Engine-Scoped environment, which is crucial for controlling extensions when security is a concern.
               * Avoid Handlebars and Mustache: They both lack the fundamental mechanisms (Custom Tags and Native Async) required for modern, integrated backend data manipulation logic. Handlebars' default global scoping also introduces unnecessary complexity for maintaining modular security.
Popularity, Adoption, and Community 🌐
This section evaluates the adoption, community size, and ecosystem of each templating engine, which are critical factors for long-term project viability and finding technical support.
Market Penetration & Adoption
Evaluation Focus: How widely used is the core engine, measured by NPM downloads and adoption by major frameworks/platforms?
Why This is Critical: Ensures long-term viability, robust community support, and ease of hiring. A large, active user base means more tutorials, fewer unpatched bugs, and easier access to developers familiar with the tool.
Option
	Primary Adoption & Status
	Analysis and Conclusion
	Handlebars
	Highest Adoption. Widely used across the Node.js ecosystem (Express.js, Koa) and historically used by major frameworks like Ember.js. Extremely high NPM download count.
	Strongest Community. The most popular choice for logic-less templating in the JavaScript world. Its maturity ensures comprehensive tooling and a massive library of custom helpers.
	Mustache
	Ubiquitous (Cross-Language). Highly adopted due to its simplicity and multi-language implementations (Java, Ruby, Python, etc.), making it popular in polyglot environments.
	Very Strong Community. While NPM downloads are generally lower than Handlebars, its conceptual popularity and cross-platform presence are unmatched. It's often the base standard for simple templates.
	Nunjucks
	Strong in Specific Niches. Popularized by Mozilla and widely used in Node.js projects that require complex layouts (due to its Jinja2 heritage) and Static Site Generators (SSGs).
	Moderate/Strong Community. The community is active, but smaller than the logic-less leaders. Its strength lies in its overlap with the massive Python/Jinja2 ecosystem, making resources easy to find.
	LiquidJS
	Platform-Driven. Primarily popularized by Shopify and heavily adopted by modern SSGs like Eleventy, and used by enterprise applications like Microsoft Power Pages.
	Rapidly Growing Community. Adoption is accelerating due to its guaranteed security features. Its user base is strong among platforms that prioritize safety and design.
	Ecosystem and Complementary Tools
Evaluation Focus: Availability of supporting packages, official documentation quality, and key integrations.
Why This is Critical: Reduces development time and project risk. A rich ecosystem means less time spent reinventing the wheel (e.g., finding pre-made filters) and better documentation for complex issues.
Option
	Ecosystem Strength
	Analysis and Conclusion
	Handlebars
	Deepest Ecosystem. Boasts a massive third-party library of custom helpers, extensions, and integration layers for virtually all popular Node.js frameworks and build tools.
	Excellent. Developers will rarely have to write unique logic, thanks to the vast collection of shared code.
	Mustache
	Wide, but Thin Ecosystem. Has many integrations across various languages, but the supporting packages are generally thin, reflecting the engine's minimal functionality.
	Fair. The tools exist, but they focus only on parsing/rendering, not providing complex filters or utilities.
	Nunjucks
	Solid Ecosystem. Benefits from strong official documentation (Mozilla) and several community-driven packages for custom tags and asynchronous helpers.
	Strong. The tooling is comprehensive enough to support complex applications, with high-quality core documentation.
	LiquidJS
	Highly Focused Ecosystem. Strong support within the e-commerce/SSG world. Tools often focus on secure integration and data handling.
	Strong. While not as vast as Handlebars, the available ecosystem is highly focused on safe, production-ready features and major platform integrations.
	Conclusions
Ranking
	Engine
	Strength of Community
	Best for...
	#1
	Handlebars
	Largest overall developer base and NPM adoption.
	Projects prioritizing ease of hiring and a massive library of custom solutions.
	#2
	Mustache
	Most portable. Strong conceptual understanding across all programming languages.
	Multi-language projects or applications targeting simplicity.
	#3
	LiquidJS
	Fastest growing, platform-driven adoption.
	Security-focused platforms and modern static site generation (SSGs).
	#4
	Nunjucks
	Strong, but niche. Popular with developers familiar with the Jinja2/Python style.
	

	Which areas should we prioritize during the evaluation?
The template is still a custom piece of logic that is executed conditionally by the template engine, so it's a matter of Extending the Built-in Functionality with careful Security controls.
Most Critical Category: Security 🛡️
Since the internal JS function runs in the backend's environment, exposing it through the template is a high-risk operation. The security evaluation is paramount.
Applicable Criterion
	Focus and Implication
	Access Control to Privileged Objects
	ABSOLUTELY CRITICAL. If the internal JS function has access to the backend's global scope, process object, or the application's configuration, a malicious template author could exploit it to leak sensitive data or perform actions they are not authorized for. The templating language must allow you to restrict what the function can see and do.
	Filter/Helper Whitelisting
	ESSENTIAL. You must be able to explicitly whitelist this function and ensure that no other potentially dangerous internal JS function is accidentally exposed to the template context. This is the main defense against Remote Code Execution (RCE).
	Sandboxing (or Isolation)
	If the templates are user-editable, the templating engine's sandboxing capabilities are tested here. Can the engine prevent the internal function from being used to escape the template context and run arbitrary code? (e.g., using constructor chains in a non-sandboxed JS environment like base Nunjucks).
	Extendability 🔌
This covers the mechanics of getting your function into the template's toolset.
Applicable Criterion
	Focus
	External Library Integration
	This criterion is about integrating a custom function into the environment. The process should be clean and allow the function to receive arguments correctly from the filter syntax.
	Scope of Extensions (Global vs. Local)
	You should prefer an engine that allows you to register the internal function on a local, isolated scope (Engine/Environment-scoped) rather than making it globally available to all templates by default.
	Built-in Functionality 🛠️
The ability to defer the execution of a function until the template demands it is a core feature of filters/helpers.
Applicable Criterion
	Shift in Focus
	Data Transformation Filters (Out-of-the-Box)
	Filters are the mechanism for this. The ease with which the engine allows you to define and register a function that is lazily executed by the filter syntax.
	Variable Assignment (BE Evaluation)
	If the internal JS function performs a calculation and the result needs to be stored for later use in the template, the engine's ability to easily accept the function's return value and assign it to a new template variable is key.
	Asynchronous Support
	Less critical, but still a factor. While many internal JS functions are synchronous, if your function needs to resolve a promise (e.g., waiting for an internal database query or an in-memory cache lookup), the engine must still support asynchronous filters or tags.
	Final Recommendation
Given the context of untrusted user input, the recommendation leans toward the safest option:
The single best alternative is LiquidJS. It minimizes the risk of catastrophic RCE/SSRF attacks, which is the necessary prerequisite for deploying any user-editable templating system. The powerful data manipulation features of Nunjucks are simply too dangerous to expose in a high-security environment without relying on complex, external sandboxing processes.
Contribution Guide
Adding New Filters and Tags
This document explains how to add custom filters or tags to the templating engine used in Kibana workflows.
Adding a new filter or tag involves updates to several components:
               1. Engine registration (server)
               2. UI validation (frontend)
               3. Autocompletion (editor)
               4. Optional open-source contribution
1. Register the Filter in the Engine
File:
src/platform/plugins/shared/workflows_execution_engine/server/templating_engine.ts
constructor() {
  this.engine = new Liquid({
    strictFilters: true,
    strictVariables: false,
  });


  // Register a custom filter
  this.engine.registerFilter('my_custom_filter', (value: unknown, ...args: unknown[]): unknown => {
    if (typeof value !== 'string') return value;
    // Implement your logic here
    return transformedValue;
  });
}
Filter Function Signature
(value: unknown, ...args: unknown[]) => unknown
Parameters
               * value — The value being filtered (left side of |)
               * args — Additional filter arguments (after :)
               * return — The transformed value
Example
this.engine.registerFilter('prefix', (value: unknown, prefix: unknown): unknown => {
  if (typeof value !== 'string' || typeof prefix !== 'string') return value;
  return `${prefix}${value}`;
});


// Usage: {{ name | prefix: "Mr. " }}

2. Add the Filter to UI Validation
The UI uses liquidjs for syntax validation.
You must register the filter in the validation instance to prevent parsing errors.
File:
 src/platform/plugins/shared/workflows_management/public/features/validate_workflow_yaml/lib/validate_liquid_template.ts
function getLiquidInstance(): Liquid {
  if (!liquidInstance) {
    liquidInstance = new Liquid({
      strictFilters: true,
      strictVariables: false,
    });


    // Register custom filters
    liquidInstance.registerFilter('json_parse', (value: unknown) => value);
    liquidInstance.registerFilter('my_custom_filter', (value: unknown) => value);
  }


  return liquidInstance;
}
Why:
Without this, users may see parsing errors when typing filters in the YAML editor.
3. Add to Autocompletion
Add the filter to the autocomplete suggestions list.
File:
 src/platform/plugins/shared/workflows_management/public/widgets/workflow_yaml_editor/lib/autocomplete/suggestions/liquid/liquid_completions.ts
export const LIQUID_FILTERS = [
  {
    name: 'my_custom_filter',
    description: 'Adds a prefix to a string',
    insertText: 'my_custom_filter: "${1:prefix}"',
    example: '{{ "value" | my_custom_filter: "prefix_" }} => "prefix_value"',
  },
];
Field Reference
Field
	Description
	name
	Filter name (must match the registration name)
	description
	A short, user-facing description
	insertText
	Snippet for autocompletion (use ${1:placeholder} syntax for arguments)
	example
	Usage example for documentation
	

4. Add Tests
Write unit tests to verify your filter logic.
File:
src/platform/plugins/shared/workflows_execution_engine/server/templating_engine.test.ts
describe('my_custom_filter', () => {
  it('adds prefix to string', () => {
    const template = '{{ name | prefix: "Mr. " }}';
    const context = { name: 'John' };
    const result = templatingEngine.render(template, context);
    expect(result).toBe('Mr. John');
  });


  it('handles non-string values gracefully', () => {
    const template = '{{ number | prefix: "prefix" }}';
    const context = { number: 123 };
    const result = templatingEngine.render(template, context);
    expect(result).toBe('123');
  });
});
5. (Recommended) Contribute to liquidjs
To make your filter available to all users, contribute it to the liquidjs project.
5.1 Open an Issue
               1. Go to liquidjs GitHub issues.
               2. Describe your filter or tag and its purpose.
               3. Explain the use case and proposed API.
               4. Wait for maintainer feedback before starting implementation.
5.2 Open a Pull Request
               1. Fork the repository.
               2. Implement your filter or tag.
               3. Add tests and documentation.
               4. Submit a PR with a clear and concise description.
5.3 Update Kibana After Merge
Once merged and released:
               1. Update the liquidjs dependency in package.json.
               2. Remove the local filter registration (now built-in).
               3. Update autocompletion definitions if needed.
               4. Adjust tests to align with upstream behavior.
6. Adding Custom Tags
Tags follow a similar process but use registerTag.
this.engine.registerTag('my_tag', {
  parse(tagToken, remainTokens) {
    // Parse tag arguments
  },
  render(context, emitter) {
    // Render tag output
  },
});
Tip: Tags are more complex than filters.
See the LiquidJS Tag Documentation for full details.
7. Best Practices
✅ Use snake_case for filter names (Liquid convention)
✅ Validate input types before applying logic
✅ Return original values on error (graceful fallback)
✅ Document filter behavior and examples
✅ Write tests for edge cases (null, undefined, invalid types)
✅ Contribute upstream when possible
8. Engine Reference
Supported Features


Category
	Features
	Liquid Core
	Variables, filters, tags, blocks, filter chaining
	Kibana Extensions
	${{ }} expression syntax, custom filters, recursive rendering, enhanced error messages
	Upgrade Guidelines
When upgrading liquidjs:
               1. Review breaking changes in the changelog
               2. Re-test all custom filters.
               3. Validate error message formats.
               4. Test template rendering in the UI.
Version Update Steps
               1. Update the liquidjs dependency in package.json.
               2. Run: yarn test templating_engine.
               3. Verify UI validation and autocompletion.
               4. Update this documentation if behavior changes.
9. References
               * Liquid Template Language
               * LiquidJS Documentation
               * LiquidJS GitHub Repository
[a]1 total reaction
Tal Borenstein reacted with 👨‍🍳 at 2025-06-04 11:29 AM
[b]It might be worth including workflow creation via API as a requirement, to support both internal development workflows and external user integrations.
[c]Are we planning to migrate workflows from other tools? Like someone having paloalto XSOAR Cortex, migrating all the playbooks to Elastic OneWorkflow.
[d]Is the name the actual identifier of the workflow, or is there a separate unique ID generated independently of the user? If not, does the user need to ensure that the name is unique?
1 total reaction
Lorena Bălan reacted with ➕ at 2025-10-28 11:50 AM
[e]Do triggers need to be defined at the time the workflow is created? Or should users be able to add triggers later?
This could be useful in use cases such as templates, where triggers might vary based on usage.
[f]What is the requirement for time-to-execution? should it be in milliseconds or can grow to seconds
[g]Not sure if this should go here or under monitoring but I would say we need to make sure that all the steps of the workflow are visible in terms of success/failure, as it would be easy to troubleshoot and identify where it failed and what was actually executed. For example, a workflow regarding malware infection could success until killing the process but failed at isolating the host. I would suggest incorporate 3 execution status: success (all steps were successful), fail (the workflow failed at the first stage) and partial execution (partially completed). Then within each workflow, each step should show as well these status as for example, within one step the connection to the 3rd party app could success but the step could fail because there was not enough privileges for the API to execute that action.
[h]This should also include number of executions for the workflow - gan give an indication of usage.
[i]We have internal tool https://github.com/elastic/edctl


"Elastic Deployment Control (edctl for short) is a tool written in Go to manage Elasticsearch & Kibana clusters. It can diff, pull & push things like index templates, users, roles, ILM policies, dashboards, watches, etc."
[j]there are also lessons learned we can draw on from detections as code when we get to this stage: https://github.com/elastic/DaC-Reference
[k]The goal is to create a central hub where users can discover and utilize both out-of-the-box templates curated by Elastic and those contributed by the community. The exact method for community submissions is something we'll need to think about some more... it could involve a direct in-product process or integration with a public repo. This distinction is important, as it will influence whether the gallery functions more like a curated app store like experiance, an open-source hub, or a combination of both. We have good examples to look at here from tines, n8n, etc.,...
[l]Having a rich library of templates OOTB is going to be a key part of our GTM...
[m]I need to think more about what requirements we need to capture around workflow content that can be provided by things like integrations. And even more big picture what Elastic's strategy is around the concept of content packs... like for example when a user is interested in Okta, we should not only enable them to connect and bring okta audit logs, etc.,, but show them what we have available interms of detection rules, dashboards, workflows, etc., 


This falls out of the scope of workflow automation, but something to think about
[n]are we planning to allow users to create their own triggers or customize them? By that I mean that I guess triggers should be defined by us like Webhook, app events, etc and then users can configure them by typing queries, conditions, etc but not allowing them to create their own triggers based on any kind of event, as we might not support them.
[o]should the conditions supported syntax be defined as part of the recruitment?
[p]need to think about what other attributes...
[q]need to think more about what other methods of authorizing the "sender" of the payload we need to support
[r]with/without input and context (e.g. for every workflow that can run with an event, a user should be able to run it manually, providing the context)
[s]maybe provider a CLI
[t]including objects (list, dict). something we are not great at
[u]a good question - can we enable the re-use of workflow with specific parameters pre-defined (similar to triggering / nesting workflow)
[v]"if" usually serves as a "skip this step" so lot of times there is no "else"
[w]else "elif"
[x]one thing we need to understand if we can do nested loops
[y]not sure if it's only a step, or that every step can declare its throttling
[z]Something I need to think about more is the role ES|QL can play in these steps...
[aa]we won't be able to know when a cred will expire but the user does, if they chose to provide this value we can build an experience where we can inform them ahead of a key's expiration
[ab]we can even link access to specific connectors based on licence (not saying we should but it's an option)
[ac]For air-gapped environments, does it make sense to request customer to create public endpoint to allow triggering workflow from external system? or we should come up with architecture that will support in house?
[ad]In general, I think there are 2 different things we're speaking about here:


1. How do we get "external systems" events in to Elastic (also referred as the "Webhook endpoint", which in Keep, is the "AIOps" part, and not the workflows part)
2. How do workflows get triggered
[ae]... note to self: need to think about the role cloud connected will play here
[af]TBD on what this "core" is that we'll retain across the deployment models
[ag]leaving a comment for whoever picks this up in:
[ah]https://github.com/orgs/elastic/projects/705/views/134?pane=issue&itemId=3606626239&issue=elastic%7Csecurity-team%7C14635
[ai]Is Data Foundation a new concept being introduced? I don't see any reference to this term in our public docs.
[aj]See https://docs.google.com/document/d/1V8KG2L1AgZ--weDKBUHvhNICKVEFvqmpwpoXNb6Mg2Y/edit?tab=t.qjg3lzzbu92n#heading=h.p7wrbdn48aww
[ak]Workflow runs successfully, however on `main` it's saying there are three validation errors in this section. Variable validation WRT respond from search API and Foreach step over an array type.
[al]If you've followed the steps above you will see nothing here. You have to run the workflow from the listing page or from an agent or alert. Executions run in the workflow tabs do not show up here.
[am]Is there a correct link for these docs or do they not exist yet?
[an]should be national-parks-data
[ao]This seems to always be evaluating to false, even after changing to `==5`, `> 2`, or moving it out of the template and comparing against a string `{{ steps.searchParks.output.hits.total.value }} == '5'`..
[ap]cc @ihor.panasiuk@elastic.co
[aq]missing `| json` afterwards
[ar]This throws a validation error fyi, `Variable item._source.title is invalid`